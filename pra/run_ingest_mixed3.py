import os
import sys
import uuid
import django
from pathlib import Path
from dotenv import load_dotenv

from langchain_community.document_loaders import Docx2txtLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# Django ì´ˆê¸°í™”
sys.path.append(os.path.abspath("."))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "config.settings")
django.setup()

from SQLDB.models import VectorMetadata

# í™˜ê²½ë³€ìˆ˜ ë° ê²½ë¡œ ì„¤ì •
load_dotenv()
DOCUMENT_DIR = Path("/home/ubuntu/policy_file")
PERSIST_DIR = "./chroma_db"
COLLECTION = "waste_policy_hf"  # "waste_vector"
os.makedirs(PERSIST_DIR, exist_ok=True)

# ì§€ì—­ ì¶”ì¶œ í•¨ìˆ˜
def extract_region_from_filename(filename: str) -> str:
    return os.path.splitext(filename)[0].split("_")[0].strip()

# ë¼ë²¨ ìë™ ë¶„ë¥˜ í•¨ìˆ˜
def classify_label(text: str) -> str:
    text = text.lower()
    if any(kw in text for kw in ["í˜íŠ¸", "í”Œë¼ìŠ¤í‹±", "í•©ì„±ìˆ˜ì§€", "pp", "pe"]):
        return "Plastic"
    elif "ìŠ¤í‹°ë¡œí¼" in text:
        return "Styrofoam"
    elif any(kw in text for kw in ["ê³ ì² ", "ì‡ ë¶™ì´", "ìŠ¤í…Œì¸ë¦¬ìŠ¤", "ì² ", "ì•Œë¯¸ëŠ„", "ê¸ˆì†"]):
        return "Metal"
    elif any(kw in text for kw in ["ìº”", "ì² ìº”", "ì•Œë£¨ë¯¸ëŠ„ìº”", "ë¶€íƒ„ê°€ìŠ¤", "ì‚´ì¶©ì œ"]):
        return "Can"
    elif any(kw in text for kw in ["ë³‘", "ìœ ë¦¬ë³‘", "ë†ì•½ë³‘", "ìŒë£Œìˆ˜ë³‘"]):
        return "Glass"
    elif any(kw in text for kw in ["ì¢…ì´", "ì‹ ë¬¸ì§€", "ì±…ì", "ìƒì", "ë‹¬ë ¥", "ë…¸íŠ¸"]):
        return "Paper"
    elif any(kw in text for kw in ["ë¹„ë‹"]):
        return "vinyl"
    elif any(kw in text for kw in ["ì¬í™œìš© ë¶ˆê°€", "ê¸°ë¦„ì¢…ì´", "ì»µë°¥", "íŒŒì‡„ì§€"]):
        return "NonRecyclable"
    else:
        return "Other"

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
hf_embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"batch_size": 5}
)

vectordb = Chroma(
    embedding_function=hf_embeddings,
    persist_directory=PERSIST_DIR,
    collection_name=COLLECTION
)

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=0,
    separators=["\n\n", "\n", " ", ""]
)

# .docx íŒŒì¼ë§Œ ëŒ€ìƒ
doc_paths = list(DOCUMENT_DIR.glob("*.docx")) + list(DOCUMENT_DIR.glob("*.DOCX"))

print(f"\nğŸ“‚ ë¬¸ì„œ íƒìƒ‰ ê²½ë¡œ: {DOCUMENT_DIR}")
print(f"ğŸ” ë¬¸ì„œ íŒŒì¼ ìˆ˜: {len(doc_paths)}")

for file_path in doc_paths:
    try:
        print(f"\nğŸ“„ Processing: {file_path.name}")
        region = extract_region_from_filename(file_path.name)

        loader = Docx2txtLoader(str(file_path))
        docs = loader.load()

        print(f"ğŸ“ loader ë°˜í™˜ íƒ€ì…: {type(docs)}")

        if isinstance(docs, str):
            docs = [Document(page_content=docs)]
        elif isinstance(docs, list):
            if all(isinstance(doc, str) for doc in docs):
                docs = [Document(page_content=doc) for doc in docs]
            elif all(isinstance(doc, Document) for doc in docs):
                pass
            else:
                raise ValueError(f"Unsupported document type in list: {type(docs[0])}")
        else:
            raise ValueError(f"Unsupported document type: {type(docs)}")

        chunks = splitter.split_documents(docs)
        texts = [chunk.page_content for chunk in chunks]
        ids = [str(uuid.uuid4()) for _ in chunks]

        documents = []
        for text, vector_id in zip(texts, ids):
            label = classify_label(text)
            metadata = {
                "type": "guideline",
                "label": label,
                "region": region,
                "source_file": file_path.name
            }
            documents.append(Document(page_content=text, metadata=metadata))

        print(f"âœ… vectordb íƒ€ì… í™•ì¸: {type(vectordb)}")
        vectordb.add_documents(documents=documents, ids=ids)

        for vector_id in ids:
            VectorMetadata.objects.create(vector_id=vector_id)

        print(f"âœ… {file_path.name}: {len(texts)} chunks ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {file_path.name} - {str(e)}")

print("\nğŸ“Œ ChromaDBì— ì €ì¥ëœ ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ í™•ì¸:")
try:
    raw = vectordb._collection.get(include=["metadatas"], limit=5)
    for i, meta in enumerate(raw["metadatas"], 1):
        print(f"ğŸ§© meta {i}: {meta}")
except Exception as e:
    print(f"âš ï¸ ë©”íƒ€ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {str(e)}")
