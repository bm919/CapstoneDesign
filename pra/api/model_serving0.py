# -*- coding: utf-8 -*-
"""model_serving.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ynT52Ycwy2rLDfyAicDuno4EUtyd_3wo
"""

# src/app.py

import os
import io
import json
from dotenv import load_dotenv

import torch
import timm
from PIL import Image
from fastapi import FastAPI, File, UploadFile, HTTPException
import torchvision.transforms as transforms

# --------------------
# Load environment variables
# --------------------
load_dotenv()  # .env 파일에서 변수 읽어오기

MODEL_PATH      = os.getenv('MODEL_PATH')
CLASS_JSON_PATH = os.getenv('CLASS_JSON_PATH')
BACKBONE_NAME   = os.getenv('BACKBONE_NAME', 'efficientnet_b0')

# --------------------
# Device 설정
# --------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --------------------
# Utilities
# --------------------
def get_inference_transform():
    """Inference 시 이미지 전처리 transform."""
    return transforms.Compose([
        transforms.Resize((320, 320)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225]),
    ])

def load_class_names(json_path: str):
    """class_names.json에서 레이블 리스트 로드."""
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"Class JSON not found at {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_model_and_labels(model_path: str, backbone_name: str, class_json_path: str):
    """
    - checkpoint 내부에 'model_state_dict', 'state_dict', 혹은 직접 state_dict만 저장된
      모든 경우를 처리합니다.
    - DataParallel로 저장된 경우 'module.' prefix도 제거합니다.
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found at {model_path}")
    ckpt = torch.load(model_path, map_location=device)
    print(">> checkpoint keys:", list(ckpt.keys()))

    # 1) state_dict 추출
    if 'model_state_dict' in ckpt:
        raw_weights = ckpt['model_state_dict']
    elif 'state_dict' in ckpt:
        raw_weights = ckpt['state_dict']
    else:
        # checkpoint 자체가 state_dict인 경우
        raw_weights = ckpt

    # 2) DataParallel로 저장되며 'module.' prefix가 붙은 경우 제거
    state_dict = {k.replace('module.', ''): v for k, v in raw_weights.items()}

    # 3) 클래스 레이블(External JSON) 로드
    class_names = load_class_names(class_json_path)
    num_classes = len(class_names)

    # 4) 모델 생성 및 state_dict 로드
    model = timm.create_model(backbone_name, pretrained=False, num_classes=num_classes)
    load_result = model.load_state_dict(state_dict, strict=False)
    if load_result.missing_keys or load_result.unexpected_keys:
        print(f"⚠️ Missing keys: {load_result.missing_keys}")
        print(f"⚠️ Unexpected keys: {load_result.unexpected_keys}")

    model.to(device).eval()
    return model, class_names

def predict_image(model, class_names, image_bytes: bytes):
    """단일 이미지 예측."""
    img = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    tensor = get_inference_transform()(img).unsqueeze(0).to(device)
    with torch.no_grad():
        logits = model(tensor)
        pred_idx = logits.argmax(dim=1).item()
        confidence = float(torch.softmax(logits, dim=1)[0, pred_idx])
        label = class_names[pred_idx]
    return {'label': label, 'confidence': confidence}

# --------------------
# FastAPI App 초기화
# --------------------
app = FastAPI(title="Waste Classification API")

# 서버 시작 시 모델·레이블 로드
try:
    model, class_names = load_model_and_labels(MODEL_PATH, BACKBONE_NAME, CLASS_JSON_PATH)
    print(f"✅ Loaded model ({BACKBONE_NAME}) with {len(class_names)} classes")
except Exception as e:
    print(f"❌ Failed to load model or labels: {e}")

@app.post('/predict')
async def predict(file: UploadFile = File(...)):
    """
    이미지 파일을 받아 분류 결과(label, confidence) 반환
    """
    try:
        img_bytes = await file.read()
        result = predict_image(model, class_names, img_bytes)
        return result
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get('/waste')
def waste_check():
    """쓰레기 분 엔드포인트 (/waste)."""
    return {"status": "ok"}

# uvicorn 실행 예시 (스크립트로 돌릴 때 주석 해제)
# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run("src.app:app", host="0.0.0.0", port=8000, reload=True)

"""1. models/ 디렉토리에 waste_classifier.pth 파일 저장
2. 가상환경 활성화하여 requirements.txt 설치
  - reqirements.txt
    - fastapi
    - uvicorn[standard]
    - torch
    - torchvision
    - timm
    - pillow
    - 모델 불러올 때 필요한 패키지를 작성하면 됨
  - pip install -r requirements.txt 작성하여 설치
3. 환경변수 설정
  - env 파일이나 쉘에서 export MODEL_PATH=models/waste_classifier.pth 실행
  - FastAPI 앱을 담은 app/model_serving.py 에서 load_model_and_labels() 함수를 호출해 서버 시작 시 모델을 메모리에 한 번 로드
  - /predict 와 /waste 엔드포인트 정의
4. 서버 실행
  - uvicorn app.model_serving:app \
    --host 0.0.0.0 \
    --port 8000 \
    --reload         # 개발 중에는 코드 변경 자동 반영
  - 배포 환경에서는 --reload를 뺀 뒤, --workers 2 이상을 주면 동시 요청 처리량이 증가함


## 동작 흐름

서버 스타트업
- load_model_and_labels() 가 호출되어 waste_classifier.pth로부터
- model_state → 모델 가중치 로드
- class_names → 라벨 리스트 로드
- GPU/CPU 디바이스 설정, model.eval()

쓰레기 분류(/waste)
- 단순 { "status": "ok" } 리턴으로 서비스 가용성 확인

예측 요청(/predict)
- 클라이언트가 multipart/form-data 형식으로 file 필드에 이미지 업로드
- predict_image() 내부에서 PIL로 이미지 디코딩 → 텐서 전처리(get_inference_transform())
- model(tensor) 순전파 → softmax → 최고 확률 라벨·신뢰도 계산
- { "label": "...", "confidence": 0.92 } 형태로 JSON 응답

## 프런트 연동 예시

### 헬스체크
curl http://your.server.ip:8000/waste

### 예측 호출
curl -X POST http://your.server.ip:8000/predict \
     -F "file=@/path/to/local/image.jpg"
"""