# run_ingest_mixed.py

import os
import sys
import uuid
import django
from pathlib import Path
from dotenv import load_dotenv

from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Django ì„¤ì •
sys.path.append(os.path.abspath("."))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "config.settings")  # í”„ë¡œì íŠ¸ì— ë§ê²Œ ìˆ˜ì •
django.setup()

from SQLDB.models import VectorMetadata
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# .env ë¡œë“œ
load_dotenv()

# íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
DOCUMENT_DIR = Path("/home/ubuntu/policy_file")  # .pdf, .docx ì €ì¥ í´ë”
PERSIST_DIR = "./chroma_db"
COLLECTION = "waste_policy_hf"

os.makedirs(PERSIST_DIR, exist_ok=True)

# ì§€ì—­ëª… ìë™ ì¶”ì¶œ í•¨ìˆ˜
def extract_region_from_filename(filename: str) -> str:
    basename = os.path.splitext(filename)[0]
    first_token = basename.split("_")[0]
    return first_token.strip()

# label ìë™ ë¶„ë¥˜ í•¨ìˆ˜
def classify_label(text: str) -> str:
    text = text.lower()
    if any(kw in text for kw in ["í˜íŠ¸", "í”Œë¼ìŠ¤í‹±", "í•©ì„±ìˆ˜ì§€", "pp", "pe"]):
        return "Plastic"
    elif "ìŠ¤í‹°ë¡œí¼" in text:
        return "Styrofoam"
    elif any(kw in text for kw in ["ê³ ì² ", "ì‡ ë¶™ì´", "ìŠ¤í…Œì¸ë¦¬ìŠ¤", "ì² ", "ì•Œë¯¸ëŠ„", "ê¸ˆì†"]):
        return "Metal"
    elif any(kw in text for kw in ["ìº”", "ì² ìº”", "ì•Œë£¨ë¯¸ëŠ„ìº”", "ë¶€íƒ„ê°€ìŠ¤", "ì‚´ì¶©ì œ"]):
        return "Can"
    elif any(kw in text for kw in ["ë³‘", "ìœ ë¦¬ë³‘", "ë†ì•½ë³‘", "ìŒë£Œìˆ˜ë³‘"]):
        return "Glass"
    elif any(kw in text for kw in ["ì¢…ì´", "ì‹ ë¬¸ì§€", "ì±…ì", "ìƒì", "ë‹¬ë ¥", "ë…¸íŠ¸"]):
        return "Paper"
    elif any(kw in text for kw in ["ì¬í™œìš© ë¶ˆê°€", "ì¬í™œìš©í’ˆì¸ì²™", "ê¸°ë¦„ì¢…ì´", "ì»µë°¥", "ë¼ë©´ ìš©ê¸°", "íŒŒì‡„ì§€"]):
        return "NonRecyclable"
    else:
        return "Other"

# ì„ë² ë”© ëª¨ë¸
hf_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ChromaDB ì—°ê²°
vectordb = Chroma(
    embedding_function=hf_embeddings,
    persist_directory=PERSIST_DIR,
    collection_name=COLLECTION
)

# ë¬¸ì„œ ë¶„í• ê¸°
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

# PDF + DOCX ìë™ íƒìƒ‰
doc_paths = list(DOCUMENT_DIR.glob("*.pdf")) + list(DOCUMENT_DIR.glob("*.docx"))

for file_path in doc_paths:
    try:
        print(f"\nğŸ“„ Processing: {file_path.name}")

        # ì§€ì—­ëª… ì¶”ì¶œ
        region = extract_region_from_filename(file_path.name)

        # íŒŒì¼ í˜•ì‹ ë¡œë” ë¶„ê¸°
        if file_path.suffix == ".pdf":
            loader = PyPDFLoader(str(file_path))
        elif file_path.suffix == ".docx":
            loader = Docx2txtLoader(str(file_path))
        else:
            print(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {file_path.name}")
            continue

        docs = loader.load()
        chunks = splitter.split_documents(docs)

        texts = [chunk.page_content for chunk in chunks]
        ids = [str(uuid.uuid4()) for _ in chunks]
        metadatas = [{
            "type": "guideline",
            "label": classify_label(chunk.page_content),
            "region": region,
            "source_file": file_path.name
        } for chunk in chunks]

        # ChromaDB ì €ì¥
        vectordb.add_documents(documents=texts, metadatas=metadatas, ids=ids)

        # PostgreSQL ì €ì¥
        for vector_id in ids:
            VectorMetadata.objects.create(vector_id=vector_id)

        print(f"âœ… {file_path.name}: {len(texts)} chunks ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {file_path.name} - {str(e)}")

vectordb.persist()
print(f"\nâœ… ì „ì²´ ingest ì™„ë£Œ: {len(doc_paths)}ê°œ ë¬¸ì„œ")

