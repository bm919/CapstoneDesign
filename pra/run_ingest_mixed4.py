import os
import sys
import uuid
import django
from pathlib import Path
from dotenv import load_dotenv

from langchain_community.document_loaders import Docx2txtLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# Django ì´ˆê¸°í™”
sys.path.append(os.path.abspath("."))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "config.settings")
django.setup()

from SQLDB.models import VectorMetadata

# í™˜ê²½ë³€ìˆ˜ ë° ê²½ë¡œ ì„¤ì •
load_dotenv()
DOCUMENT_DIR = Path("/home/ubuntu/policy_file")
PERSIST_DIR = "./chroma_db"
COLLECTION = "waste_vectors"
os.makedirs(PERSIST_DIR, exist_ok=True)

# ----------- ì§€ì—­ ì¶”ì¶œ í•¨ìˆ˜ -----------
def extract_region_from_filename(filename: str) -> str:
    return os.path.splitext(filename)[0].split("_")[0].strip()

# ----------- í´ë˜ìŠ¤ëª… ê¸°ì¤€ ë¼ë²¨ë§ ----------
CLASS_LABEL_KEYWORDS = {
    "beverage_can": ["ìŒë£Œìˆ˜ìº”", "ë§¥ì£¼ìº”", "ìº”ë¥˜", "ìŒë£Œ/ì£¼ë¥˜ìº”", "beverage can"],
    "canned_can": ["í†µì¡°ë¦¼ìº”", "ì‹ë£Œí’ˆìº”", "ì‹í’ˆìº”", "canned can"],
    "glass_beer_bottle": ["ë§¥ì£¼ë³‘", "beer bottle", "glass beer bottle"],
    "glass_clear_bottle": ["íˆ¬ëª… ìœ ë¦¬ë³‘", "ìœ ë¦¬ë³‘", "clear bottle", "glass clear bottle"],
    "glass_tableware": ["ìœ ë¦¬ ì‹ê¸°", "ìœ ë¦¬ì‹ê¸°", "glass tableware"],
    "metal_pot": ["ëƒ„ë¹„", "ê¸ˆì†ëƒ„ë¹„", "metal pot"],
    "paper_book": ["ì±…ì", "ë…¸íŠ¸", "ì±…", "ì¡ì§€", "ê³µì±…", "paper book"],
    "paper_box": ["ì¢…ì´ë°•ìŠ¤", "ì¢…ì´íŒ©", "ê³¨íŒì§€", "ìƒì", "paper box"],
    "paper_news": ["ì‹ ë¬¸ì§€", "paper news"],
    "plastic_bottle": ["í˜íŠ¸ë³‘", "í”Œë¼ìŠ¤í‹±ë³‘", "plastic bottle"],
    "plastic_crushed": ["ì••ì°© í”Œë¼ìŠ¤í‹±", "ì••ì°©", "crushed plastic", "plastic crushed"],
    "plastic_cup_container": ["í”Œë¼ìŠ¤í‹± ì»µ", "í”Œë¼ìŠ¤í‹±ìš©ê¸°", "í”Œë¼ìŠ¤í‹± ì»¨í…Œì´ë„ˆ", "plastic cup", "plastic container", "plastic_cup_container"],
    "vinyl": ["ë¹„ë‹", "ë¹„ë‹ë´‰íˆ¬", "í¬ì¥ ë¹„ë‹", "vinyl"],
}

def classify_label(text: str) -> str:
    text = text.lower()
    for class_name, keywords in CLASS_LABEL_KEYWORDS.items():
        if any(kw in text for kw in keywords):
            return class_name
    return "Other"

# ----------- ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” -----------
hf_embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"batch_size": 5}
)

vectordb = Chroma(
    embedding_function=hf_embeddings,
    persist_directory=PERSIST_DIR,
    collection_name=COLLECTION
)

# ----------- chunk splitter -----------
splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,   # ì¢€ ë” ì„¸ë¶„í™” ì›í•  ì‹œ ë” ì¤„ì´ê¸°
    chunk_overlap=0,
    separators=[
        "\n\n", "\n", "end"    ]
)

# .docx íŒŒì¼ë§Œ ëŒ€ìƒ
#doc_paths = list(DOCUMENT_DIR.glob("*.docx")) + list(DOCUMENT_DIR.glob("*.DOCX"))
doc_paths = [DOCUMENT_DIR / "ë‚¨ì–‘ì£¼ì‹œ_ë¶„ë¦¬ìˆ˜ê±°ê·œì •.docx"]

print(f"\nğŸ“‚ ë¬¸ì„œ íƒìƒ‰ ê²½ë¡œ: {DOCUMENT_DIR}")
print(f"ğŸ” ë¬¸ì„œ íŒŒì¼ ìˆ˜: {len(doc_paths)}")

for file_path in doc_paths:
    try:
        print(f"\nğŸ“„ Processing: {file_path.name}")
        region = extract_region_from_filename(file_path.name)

        loader = Docx2txtLoader(str(file_path))
        docs = loader.load()

        # --- ì „ì²˜ë¦¬: ì£¼ìš” ë¶„í•  í‚¤ì›Œë“œë¡œ ì¤„ë°”ê¿ˆ ê°•ì œ ì‚½ì…
        def doc_pre_split(text):
            for kw in [
                "ë°°ì¶œë°©ë²•", "ë°°ì¶œìš”ë ¹", "í•´ë‹¹í’ˆëª©", "ë¹„í•´ë‹¹í’ˆëª©",
                "ì¢…ì´ë¥˜", "ìœ ë¦¬ë³‘", "ê¸ˆì†ìº”", "ë¹„ë‹", "ìŠ¤í‹°ë¡œí¼", "í”Œë¼ìŠ¤í‹±",
                "ì¢…ì´íŒ©", "ì±…ì", "ìƒìë¥˜", "í¬ì¥ì§€", "ìº”ë¥˜", "ê³ ì² ", "ì „ìì œí’ˆ", "í˜•ê´‘ë“±"
            ]:
                text = text.replace(kw, f"\n\n{kw}")
            return text

        if isinstance(docs, str):
            docs = [Document(page_content=doc_pre_split(docs))]
        elif isinstance(docs, list):
            if all(isinstance(doc, str) for doc in docs):
                docs = [Document(page_content=doc_pre_split(doc)) for doc in docs]
            elif all(isinstance(doc, Document) for doc in docs):
                pass
            else:
                raise ValueError(f"Unsupported document type in list: {type(docs[0])}")
        else:
            raise ValueError(f"Unsupported document type: {type(docs)}")

        # --- chunk ë¶„í•  ---
        chunks = splitter.split_documents(docs)
        print(f"ë¶„í• ëœ chunk ìˆ˜: {len(chunks)}")
        for idx, chunk in enumerate(chunks[:5], 1):
            print(f"\nchunk {idx} ìƒ˜í”Œ:\n{chunk.page_content[:200]}\n---")

        # --- ë¼ë²¨ë§ ë° DB ì €ì¥ ---
        ids = [str(uuid.uuid4()) for _ in chunks]
        documents = []
        for chunk, vector_id in zip(chunks, ids):
            label = classify_label(chunk.page_content)
            metadata = {
                "type": "guideline",
                "label": label,
                "region": region,
                "source_file": file_path.name
            }
            documents.append(Document(page_content=chunk.page_content, metadata=metadata))

        print(f"âœ… vectordb íƒ€ì… í™•ì¸: {type(vectordb)}")
        vectordb.add_documents(documents=documents, ids=ids)

        for vector_id in ids:
            VectorMetadata.objects.create(vector_id=vector_id)

        print(f"âœ… {file_path.name}: {len(documents)} chunks ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {file_path.name} - {str(e)}")

print("\nğŸ“Œ ChromaDBì— ì €ì¥ëœ ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ í™•ì¸:")
try:
    raw = vectordb._collection.get(include=["metadatas", "documents"], limit=5)
    for i, (meta, doc) in enumerate(zip(raw["metadatas"], raw["documents"]), 1):
        print(f"\nğŸ§© meta {i}: {meta}")
        print(f"ğŸ“ doc {i}: {doc[:120]}...")
except Exception as e:
    print(f"âš ï¸ ë©”íƒ€ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {str(e)}")

