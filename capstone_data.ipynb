{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246a2eb0",
   "metadata": {},
   "source": [
    "###  **1. Îç∞Ïù¥ÌÑ∞ Ïõπ ÌÅ¨Î°§ÎßÅ ÏΩîÎìú**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f06fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from PIL import Image, ImageStat\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resize_size = (512, 512)\n",
    "\n",
    "def is_blurry(image, threshold=100):\n",
    "    try:\n",
    "        img = np.array(image)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        lap = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        return lap < threshold\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_mostly_black(image, threshold=15):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    avg_brightness = sum(stat.mean) / len(stat.mean)\n",
    "    return avg_brightness < threshold\n",
    "\n",
    "def resize_images_in_folder(folder_path, size):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath).convert(\"RGB\") as img:\n",
    "                if is_blurry(img) or is_mostly_black(img):\n",
    "                    os.remove(fpath)\n",
    "                    continue\n",
    "                img = img.resize(size)\n",
    "                new_fpath = os.path.splitext(fpath)[0] + \".jpg\"\n",
    "                img.save(new_fpath, \"JPEG\")\n",
    "                if fpath != new_fpath:\n",
    "                    os.remove(fpath)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Î¶¨ÏÇ¨Ïù¥Ïßï Ïã§Ìå®: {fname} ‚Üí {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "def remove_duplicate_images(folder_path, hash_size=16):\n",
    "    seen_hashes = set()\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                img_hash = imagehash.phash(img, hash_size=hash_size)\n",
    "            if img_hash in seen_hashes:\n",
    "                os.remove(fpath)\n",
    "                print(f\"üóëÔ∏è Ï§ëÎ≥µ Ï†úÍ±∞: {fname}\")\n",
    "            else:\n",
    "                seen_hashes.add(img_hash)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Ìï¥Ïãú Ïã§Ìå®: {fname} ‚Üí {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "def compute_hashes_in_folder(folder_path, hash_size=16):\n",
    "    hashes = set()\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                h = imagehash.phash(img, hash_size=hash_size)\n",
    "                hashes.add(str(h))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Ìï¥Ïãú Í≥ÑÏÇ∞ Ïã§Ìå®: {fname} ‚Üí {e}\")\n",
    "    return hashes\n",
    "\n",
    "def crawl_new_images_for_class(class_name, search_queries, main_folder, temp_folder, max_num=50, hash_size=16):\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    existing_hashes = compute_hashes_in_folder(main_folder, hash_size=hash_size)\n",
    "\n",
    "    for query in search_queries:\n",
    "        query_folder = os.path.join(temp_folder, query.replace(\" \", \"_\"))\n",
    "        os.makedirs(query_folder, exist_ok=True)\n",
    "        print(f\"[{class_name}] ÏÉàÎ°úÏö¥ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Î°§ÎßÅ Ï§ë - Í≤ÄÏÉâÏñ¥: '{query}'\")\n",
    "        crawler = GoogleImageCrawler(storage={\"root_dir\": query_folder})\n",
    "        crawler.crawl(\n",
    "            keyword=query,\n",
    "            max_num=max_num,\n",
    "            filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "        )\n",
    "        resize_images_in_folder(query_folder, resize_size)\n",
    "\n",
    "        for fname in os.listdir(query_folder):\n",
    "            fpath = os.path.join(query_folder, fname)\n",
    "            try:\n",
    "                with Image.open(fpath) as img:\n",
    "                    h = str(imagehash.phash(img, hash_size=hash_size))\n",
    "                if h in existing_hashes:\n",
    "                    os.remove(fpath)\n",
    "                    print(f\"Ï§ëÎ≥µ Ïù¥ÎØ∏ÏßÄ Ï†úÍ±∞Îê®: {fname}\")\n",
    "                else:\n",
    "                    dest_path = os.path.join(main_folder, fname)\n",
    "                    shutil.move(fpath, dest_path)\n",
    "                    existing_hashes.add(h)\n",
    "            except Exception as e:\n",
    "                print(f\"Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ïò§Î•ò {fname}: {e}\")\n",
    "\n",
    "        if not os.listdir(query_folder):\n",
    "            os.rmdir(query_folder)\n",
    "        time.sleep(5)\n",
    "\n",
    "    remove_duplicate_images(main_folder, hash_size=hash_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÌÅ¨Î°§ÎßÅ Í≤ÄÏÉâÏñ¥ ‚îÄ‚îÄ‚îÄ\n",
    "crawl_targets = {\n",
    "    \"wood\": [\"wood\", \"plywood pieces\", \"furniture wood pieces\"],\n",
    "    \"food_waste\": [\n",
    "        \"egg shells\", \"duck egg shells\", \"quail egg shells\", \"ostrich egg shells\",\n",
    "        \"walnut shells\", \"peanut shells\", \"chestnut shells\", \"acorn shells\",\n",
    "        \"pineapple peels\", \"corn husks\", \"corn cobs\",\n",
    "        \"green tea leaves\", \"herbal medicine residue\",\n",
    "        \"pork bones\", \"beef bones\", \"chicken bones\", \"fish bones\",\n",
    "        \"clam shells\", \"crab shells\", \"lobster shells\",\n",
    "        \"rice husks\", \"food waste\", \"onion peels\"\n",
    "    ],\n",
    "    \"general_waste\": [\"ceramic dishes\", \"porcelain items\"],\n",
    "    \"paper\": [\"paper packs\", \"paper cups\", \"newspapers\", \"books\", \"notebooks\", \"cardboard boxes\"],\n",
    "    \"glass\": [\"glass bottles\", \"broken glass\"],\n",
    "    \"can\": [\"steel cans\", \"aluminum cans\", \"butane gas cans\", \"pesticide cans\"],\n",
    "    \"plastic\": [\"clear PET bottles\", \"colored PET bottles\", \"plastic bags\"],\n",
    "    \"styrofoam\": [\"styrofoam\", \"contaminated styrofoam\"],\n",
    "    \"battery\": [\"batteries\", \"AA batteries\", \"AAA batteries\"],\n",
    "    \"electronics\": [\"TVs\", \"refrigerators\", \"washing machines\", \"air conditioners\", \"computers\", \"mobile phones\"],\n",
    "    \"lighting\": [\"fluorescent lamps\"],\n",
    "    \"metal\": [\"scrap metal\", \"iron pipes\"],\n",
    "    \"clothing\": [\"clothes\", \"old clothes\"]\n",
    "}\n",
    "\n",
    "modified_crawl_targets = {\n",
    "    category: [f\"{query} waste img\" for query in queries]\n",
    "    for category, queries in crawl_targets.items()\n",
    "}\n",
    "\n",
    "base_dir = r\"C:\\Users\\Administrator\\Downloads\\end-to-end-image-scraper\\downloaded_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_images_by_category(crawl_targets, base_dir, max_per_variant=100):\n",
    "    for category, queries in crawl_targets.items():\n",
    "        category_folder = os.path.join(base_dir, category)\n",
    "        os.makedirs(category_folder, exist_ok=True)\n",
    "        print(f\"\\n=== Ïπ¥ÌÖåÍ≥†Î¶¨: {category} ===\")\n",
    "        for query in tqdm(queries, desc=f\"Processing {category}\", unit=\"query\"):\n",
    "            sub_folder = os.path.join(category_folder, query.replace(\" \", \"_\"))\n",
    "            os.makedirs(sub_folder, exist_ok=True)\n",
    "            print(f\"üì¶ ÌÅ¨Î°§ÎßÅ: '{query}' ‚Üí {sub_folder}\")\n",
    "            crawler = GoogleImageCrawler(storage={\"root_dir\": sub_folder})\n",
    "            crawler.crawl(\n",
    "                keyword=query,\n",
    "                max_num=max_per_variant,\n",
    "                filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "            )\n",
    "            resize_images_in_folder(sub_folder, resize_size)\n",
    "            remove_duplicate_images(sub_folder)\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584db918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_per_class(base_dir):\n",
    "    counts = {}\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        total = 0\n",
    "        for root, _, files in os.walk(category_path):\n",
    "            total += len([f for f in files if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "        counts[category] = total\n",
    "    return counts\n",
    "\n",
    "def visualize_image_counts(base_dir):\n",
    "    counts = count_images_per_class(base_dir)\n",
    "    categories = list(counts.keys())\n",
    "    image_counts = list(counts.values())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(categories, image_counts, color='skyblue')\n",
    "    plt.xlabel(\"Ïπ¥ÌÖåÍ≥†Î¶¨\")\n",
    "    plt.ylabel(\"Ïù¥ÎØ∏ÏßÄ Í∞úÏàò\")\n",
    "    plt.title(\"Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ (ÌïÑÏöî Ïãú Ï£ºÏÑù Ìï¥Ï†ú)\n",
    "# crawl_images_by_category(modified_crawl_targets, base_dir, max_per_variant=300)\n",
    "# visualize_image_counts(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff7458",
   "metadata": {},
   "source": [
    "### **2. Î∂ÄÏ°±Ìïú ÌÅ¥ÎûòÏä§ Ïû¨ÌÅ¨Î°§ÎßÅ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image, ImageStat\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import cv2\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "\n",
    "# Í∏∞Ï°¥ resize, is_blurry, is_mostly_black, remove_duplicate_images Ìï®ÏàòÎäî ÏúÑÏóêÏÑú Ï†ïÏùò\n",
    "def get_existing_hashes(base_dir):\n",
    "    existing_hashes = set()\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(root, fname)\n",
    "            try:\n",
    "                with Image.open(fpath) as img:\n",
    "                    h = imagehash.phash(img)\n",
    "                    existing_hashes.add(h)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return existing_hashes\n",
    "\n",
    "def remove_duplicate_with_existing(folder_path, existing_hashes, hash_size=16):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                img_hash = imagehash.phash(img, hash_size=hash_size)\n",
    "            if img_hash in existing_hashes:\n",
    "                os.remove(fpath)\n",
    "                print(f\"üóëÔ∏è Í∏∞Ï°¥ Ï§ëÎ≥µ Ï†úÍ±∞: {fname}\")\n",
    "            else:\n",
    "                existing_hashes.add(img_hash)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Ìï¥Ïãú Ïã§Ìå®: {fname} ‚Üí {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "def get_class_counts(base_dir):\n",
    "    counts = {}\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            num_images = len([\n",
    "                f for f in os.listdir(category_path)\n",
    "                if os.path.isfile(os.path.join(category_path, f))\n",
    "            ])\n",
    "            counts[category] = num_images\n",
    "    return counts\n",
    "\n",
    "def recrawl_category_with_modifier(category, modifier, save_path, max_per_variant=50):\n",
    "    search_query = f\"{modifier} {category}\"\n",
    "    sub_folder = f\"{category.replace(' ', '_')}_{modifier.replace(' ', '_')}\"\n",
    "    output_path = os.path.join(save_path, sub_folder)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    print(f\"üì¶ Ïû¨ÌÅ¨Î°§ÎßÅ: {search_query} ‚Üí {output_path}\")\n",
    "    crawler = GoogleImageCrawler(storage={\"root_dir\": output_path})\n",
    "    crawler.crawl(\n",
    "        keyword=search_query,\n",
    "        max_num=max_per_variant,\n",
    "        filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "    )\n",
    "\n",
    "    resize_images_in_folder(output_path, resize_size)\n",
    "    remove_duplicate_with_existing(output_path, existing_hashes)\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïû¨ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ\n",
    "existing_hashes = get_existing_hashes(base_dir)\n",
    "for category in os.listdir(base_dir):\n",
    "    category_path = os.path.join(base_dir, category)\n",
    "    if os.path.isdir(category_path):\n",
    "        remove_duplicate_with_existing(category_path, existing_hashes)\n",
    "\n",
    "class_counts = get_class_counts(base_dir)\n",
    "print(\"ÌòÑÏû¨ ÌÅ¥ÎûòÏä§Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò:\")\n",
    "for cat, count in class_counts.items():\n",
    "    print(f\"{cat}: {count}\")\n",
    "\n",
    "# Î∂ÄÏ°± ÌÅ¥ÎûòÏä§ Ïû¨ÌÅ¨Î°§ÎßÅ\n",
    "minimum_images = 200\n",
    "extra_modifiers = [\"recycled\", \"old\", \"broken\", \"dirty\", \"used\", \"disposed\"]\n",
    "\n",
    "for category, count in class_counts.items():\n",
    "    if count < minimum_images:\n",
    "        print(f\"Ïπ¥ÌÖåÍ≥†Î¶¨ '{category}'Ïùò Ïù¥ÎØ∏ÏßÄÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§ ({count}Í∞ú). Ïû¨ÌÅ¨Î°§ÎßÅÏùÑ ÏãúÏûëÌï©ÎãàÎã§.\")\n",
    "        category_folder = os.path.join(base_dir, category)\n",
    "        for modifier in extra_modifiers:\n",
    "            recrawl_category_with_modifier(category, modifier, category_folder, max_per_variant=50)\n",
    "\n",
    "# ÏµúÏ¢Ö Ïù¥ÎØ∏ÏßÄ Ïàò Ï∂úÎ†•\n",
    "final_counts = get_class_counts(base_dir)\n",
    "print(\"ÏµúÏ¢Ö ÌÅ¥ÎûòÏä§Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò:\")\n",
    "for cat, count in final_counts.items():\n",
    "    print(f\"{cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040eb8ae",
   "metadata": {},
   "source": [
    "**Îç∞Ïù¥ÌÑ∞ ÌÅ¨Î°§ÎßÅÏùÄ Ï†ÄÏûëÍ∂å Î¨∏Ï†úÎ°ú Ïù∏Ìï¥ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÍ∏∞Î°ú Í≤∞Ï†ï**\\\n",
    "**-> Îã§ÏñëÌïú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ïù¥Ïö©ÌïòÏó¨ Î™®Îç∏ ÌïôÏäµ ÏßÑÌñâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f679f9",
   "metadata": {},
   "source": [
    "### **ÏÇ¨Ï†Ñ ÌïôÏäµ Î™®Îç∏ÏùÑ ÌÜµÌï¥ Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÎ•ò**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a154a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ImageNet class label Îã§Ïö¥Î°úÎìú\n",
    "url, filename = (\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\", \"imagenet_class_index.json\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "with open('imagenet_class_index.json') as f:\n",
    "    class_idx = json.load(f)\n",
    "idx2label = {int(k): v[1] for k, v in class_idx.items()}\n",
    "\n",
    "# Î∂ÑÎ•ò ÎåÄÏÉÅ Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî ÏßÄÏ†ï\n",
    "image_dir = r\"C:\\Users\\Administrator\\Downloads\\capstone data\\DATASET\\TRAIN\\R\"\n",
    "results = []\n",
    "\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        img_path = os.path.join(image_dir, image_file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        probs = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        confidence, pred_idx = torch.max(probs, dim=0)\n",
    "        label = idx2label[pred_idx.item()]\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": image_file,\n",
    "            \"predicted_label\": label,\n",
    "            \"confidence\": confidence.item()\n",
    "        })\n",
    "\n",
    "# ÏòàÏ∏° Í≤∞Í≥º Ï†ÄÏû•\n",
    "pd.DataFrame(results).to_csv(\"predictions.csv\", index=False)\n",
    "with open(\"predictions.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae602bc6",
   "metadata": {},
   "source": [
    "**Îç∞Ïù¥ÌÑ∞ÏÖã ÎÇ¥Ïùò Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ï†ïÌôïÌïòÍ≤å Î∂ÑÎ•òÌïòÏßÄ Î™ªÌï®**\\\n",
    "**Îî∞ÎùºÏÑú ÏßÅÏ†ë Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•òÌïòÏó¨ Î™®Îç∏Ïùò Ï†ïÌôïÎèÑÎ•º ÎÜíÏù¥Î†§ ÎÖ∏Î†•**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10f7fe",
   "metadata": {},
   "source": [
    "### **Ï°∞ÏÇ¨Ìïú Îç∞Ïù¥ÌÑ∞ÏÖã Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1bbe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fed6e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR    = r\"C:\\Users\\Administrator\\Desktop\\data3\\WashingMachine\"\n",
    "CLEAN_DIR   = r\"C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\"\n",
    "TARGET_SIZE = 224\n",
    "PADDING     = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4624805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å: C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\n",
      "\n",
      "=== ÏÉòÌîå ÌååÏùº Î™©Î°ù (ÏµúÎåÄ 10Í∞ú) ===\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_0.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_1.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_10.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_100.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_101.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_102.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_104.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_107.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_109.png\n",
      "C:\\Users\\Administrator\\Desktop\\WashingMachine_dataset\\Washing_Machine_11.png\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 1. Î¨¥Í≤∞ÏÑ± Í≤ÄÏÇ¨ & Ï§ëÎ≥µ Ï†úÍ±∞ (MD5) ‚îÄ‚îÄ‚îÄ\n",
    "seen_hashes = set()\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for fname in files:\n",
    "        src_path = os.path.join(root, fname)\n",
    "        try:\n",
    "            data = open(src_path, \"rb\").read()\n",
    "            h = hashlib.md5(data).hexdigest()\n",
    "        except Exception:\n",
    "            os.remove(src_path)\n",
    "            continue\n",
    "        if h in seen_hashes:\n",
    "            os.remove(src_path)\n",
    "        else:\n",
    "            seen_hashes.add(h)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2. ÌôïÏû•Ïûê Í≤ÄÏ¶ù ‚îÄ‚îÄ‚îÄ\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for fname in files:\n",
    "        if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "            os.remove(os.path.join(root, fname))\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3. Ï†ÑÏ≤òÎ¶¨ & Ï†ÄÏû• (PNG) ‚îÄ‚îÄ‚îÄ\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    rel_dir = os.path.relpath(root, DATA_DIR)\n",
    "    dst_dir = os.path.join(CLEAN_DIR, rel_dir)\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    for fname in files:\n",
    "        src_p = os.path.join(root, fname)\n",
    "        base, _ = os.path.splitext(fname)\n",
    "        dst_p = os.path.join(dst_dir, base + \".png\")\n",
    "\n",
    "        try:\n",
    "            img = Image.open(src_p).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Resize: ÏßßÏùÄ Î≥ÄÌòïÎ©¥ ‚Üí TARGET_SIZE + 32\n",
    "        short = min(img.size)\n",
    "        scale = (TARGET_SIZE + 32) / short\n",
    "        img = img.resize((int(img.width * scale), int(img.height * scale)), Image.BILINEAR)\n",
    "\n",
    "        # Pad or Center Crop\n",
    "        if PADDING:\n",
    "            dw = max(TARGET_SIZE - img.width, 0)\n",
    "            dh = max(TARGET_SIZE - img.height, 0)\n",
    "            pad = (dw // 2, dh // 2, dw - dw // 2, dh - dh // 2)\n",
    "            img = ImageOps.expand(img, pad, fill=(0, 0, 0))\n",
    "        img = ImageOps.fit(img, (TARGET_SIZE, TARGET_SIZE), method=Image.BILINEAR, centering=(0.5, 0.5))\n",
    "\n",
    "        img.save(dst_p, format=\"PNG\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4. Í≤∞Í≥º ÌôïÏù∏ ‚îÄ‚îÄ‚îÄ\n",
    "print(\"‚öôÔ∏è Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å:\", CLEAN_DIR)\n",
    "print(\"\\n=== ÏÉòÌîå ÌååÏùº Î™©Î°ù (ÏµúÎåÄ 10Í∞ú) ===\")\n",
    "count = 0\n",
    "for root, _, files in os.walk(CLEAN_DIR):\n",
    "    for f in files:\n",
    "        print(os.path.join(root, f))\n",
    "        count += 1\n",
    "        if count >= 10:\n",
    "            break\n",
    "    if count >= 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e8db9",
   "metadata": {},
   "source": [
    "### **ChromaDB Î¨∏ÏÑú ÏûÑÎ≤†Îî© Î∞è Í≤ÄÏÉâ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïÑÏöî Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò (Ï£ºÏÑù Ìï¥Ï†úÌïòÏó¨ ÏµúÏ¥à 1Ìöå Ïã§Ìñâ)\n",
    "# !pip install -U langchain-community\n",
    "# !pip install langchain chromadb unstructured sentence-transformers docarray\n",
    "\n",
    "import os\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader, PyPDFLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import BaseRetriever\n",
    "\n",
    "os.makedirs(\"chroma_db\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_to_chromadb(\n",
    "    source_path: str,\n",
    "    persist_directory: str = \"./chroma_db\",\n",
    "    collection_name: str = \"waste_policy\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    ") -> Chroma:\n",
    "    # ÌååÏùº Î°úÎçî ÏÑ†ÌÉù\n",
    "    if source_path.lower().endswith(\".docx\"):\n",
    "        loader = UnstructuredWordDocumentLoader(source_path)\n",
    "    elif source_path.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(source_path)\n",
    "    else:\n",
    "        raise ValueError(\"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. (.docx, .pdf Îßå Í∞ÄÎä•)\")\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Î¨∏ÏÑú Ï™ºÍ∞úÍ∏∞\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\"Heading 1\", \"Heading 2\", \"Heading 3\"],\n",
    "        max_chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # ÏûÑÎ≤†Îî© Î∞è Ï†ÄÏû•\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    return vectordb\n",
    "\n",
    "def get_simple_retriever(vectordb: Chroma, k: int = 4) -> BaseRetriever:\n",
    "    return vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ingest_to_chromadb(\"document_data.pdf\")\n",
    "retriever = get_simple_retriever(db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
