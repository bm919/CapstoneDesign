{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731b1641",
   "metadata": {},
   "source": [
    "## 데이터 웹 크롤링 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from PIL import Image, ImageStat\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resize_size = (512, 512)\n",
    "\n",
    "# --- 이미지 품질 및 필터 함수 ---\n",
    "def is_blurry(image, threshold=100):\n",
    "    try:\n",
    "        img = np.array(image)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        lap = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        return lap < threshold\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_mostly_black(image, threshold=15):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    avg_brightness = sum(stat.mean) / len(stat.mean)\n",
    "    return avg_brightness < threshold\n",
    "\n",
    "def resize_images_in_folder(folder_path, size):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath).convert(\"RGB\") as img:\n",
    "                if is_blurry(img) or is_mostly_black(img):\n",
    "                    os.remove(fpath)\n",
    "                    continue\n",
    "                img = img.resize(size)\n",
    "                new_fpath = os.path.splitext(fpath)[0] + \".jpg\"\n",
    "                img.save(new_fpath, \"JPEG\")\n",
    "                if fpath != new_fpath:\n",
    "                    os.remove(fpath)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 리사이징 실패: {fname} → {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "def remove_duplicate_images(folder_path, hash_size=16):\n",
    "    seen_hashes = set()\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                img_hash = imagehash.phash(img, hash_size=hash_size)\n",
    "            if img_hash in seen_hashes:\n",
    "                os.remove(fpath)\n",
    "                print(f\"🗑️ 중복 제거: {fname}\")\n",
    "            else:\n",
    "                seen_hashes.add(img_hash)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 해시 실패: {fname} → {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "# --- 폴더 내 이미지의 해시 값들을 계산 ---\n",
    "def compute_hashes_in_folder(folder_path, hash_size=16):\n",
    "    hashes = set()\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                h = imagehash.phash(img, hash_size=hash_size)\n",
    "                hashes.add(str(h))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 해시 계산 실패: {fname} → {e}\")\n",
    "    return hashes\n",
    "\n",
    "# ─── 부족한 클래스에 대해 재크롤링 (임시 폴더에 크롤링 후 중복 체크) ─────────────────────────\n",
    "def crawl_new_images_for_class(class_name, search_queries, main_folder, temp_folder, max_num=50, hash_size=16):\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    # 기존 이미지들의 해시 계산\n",
    "    existing_hashes = compute_hashes_in_folder(main_folder, hash_size=hash_size)\n",
    "    \n",
    "    for query in search_queries:\n",
    "        query_folder = os.path.join(temp_folder, query.replace(\" \", \"_\"))\n",
    "        os.makedirs(query_folder, exist_ok=True)\n",
    "        print(f\"[{class_name}] 새로운 이미지 크롤링 중 - 검색어: '{query}'\")\n",
    "        crawler = GoogleImageCrawler(storage={\"root_dir\": query_folder})\n",
    "        crawler.crawl(\n",
    "            keyword=query,\n",
    "            max_num=max_num,\n",
    "            filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "        )\n",
    "        \n",
    "        resize_images_in_folder(query_folder, resize_size)\n",
    "        \n",
    "        for fname in os.listdir(query_folder):\n",
    "            fpath = os.path.join(query_folder, fname)\n",
    "            try:\n",
    "                with Image.open(fpath) as img:\n",
    "                    h = str(imagehash.phash(img, hash_size=hash_size))\n",
    "                if h in existing_hashes:\n",
    "                    os.remove(fpath)\n",
    "                    print(f\"중복 이미지 제거됨: {fname}\")\n",
    "                else:\n",
    "                    dest_path = os.path.join(main_folder, fname)\n",
    "                    shutil.move(fpath, dest_path)\n",
    "                    existing_hashes.add(h)\n",
    "            except Exception as e:\n",
    "                print(f\"이미지 처리 오류 {fname}: {e}\")\n",
    "        \n",
    "        if not os.listdir(query_folder):\n",
    "            os.rmdir(query_folder)\n",
    "        time.sleep(5)\n",
    "\n",
    "    remove_duplicate_images(main_folder, hash_size=hash_size)\n",
    "\n",
    "# ─── 크롤 대상 클래스 (검색어 리스트) ──────────────────────────────────────────────\n",
    "crawl_targets = {\n",
    "    \"wood\": [\"wood\", \"plywood pieces\", \"furniture wood pieces\"],\n",
    "    \"food_waste\": [\n",
    "        \"egg shells\", \"duck egg shells\", \"quail egg shells\", \"ostrich egg shells\",\n",
    "        \"walnut shells\", \"peanut shells\", \"chestnut shells\", \"acorn shells\",\n",
    "        \"pineapple peels\", \"corn husks\", \"corn cobs\",\n",
    "        \"green tea leaves\", \"herbal medicine residue\",\n",
    "        \"pork bones\", \"beef bones\", \"chicken bones\", \"fish bones\",\n",
    "        \"clam shells\", \"crab shells\", \"lobster shells\",\n",
    "        \"rice husks\", \"food waste\", \"onion peels\"\n",
    "    ],\n",
    "    \"general_waste\": [\"ceramic dishes\", \"porcelain items\"],\n",
    "    \"paper\": [\"paper packs\", \"paper cups\", \"newspapers\", \"books\", \"notebooks\", \"cardboard boxes\"],\n",
    "    \"glass\": [\"glass bottles\", \"broken glass\"],\n",
    "    \"can\": [\"steel cans\", \"aluminum cans\", \"butane gas cans\", \"pesticide cans\"],\n",
    "    \"plastic\": [\"clear PET bottles\", \"colored PET bottles\", \"plastic bags\"],\n",
    "    \"styrofoam\": [\"styrofoam\", \"contaminated styrofoam\"],\n",
    "    \"battery\": [\"batteries\", \"AA batteries\", \"AAA batteries\"],\n",
    "    \"electronics\": [\"TVs\", \"refrigerators\", \"washing machines\", \"air conditioners\", \"computers\", \"mobile phones\"],\n",
    "    \"lighting\": [\"fluorescent lamps\"],\n",
    "    \"metal\": [\"scrap metal\", \"iron pipes\"],\n",
    "    \"clothing\": [\"clothes\", \"old clothes\"]\n",
    "}\n",
    "\n",
    "# 각 검색어 문자열의 끝에 \" waste img\"를 추가한 새로운 딕셔너리 생성\n",
    "modified_crawl_targets = {\n",
    "    category: [f\"{query} waste img\" for query in queries]\n",
    "    for category, queries in crawl_targets.items()\n",
    "}\n",
    "\n",
    "print(modified_crawl_targets)\n",
    "\n",
    "base_dir = r\"C:\\Users\\Administrator\\Downloads\\end-to-end-image-scraper\\downloaded_images\"\n",
    "\n",
    "# ─── 각 카테고리별 이미지 크롤링 함수 (진행 바 포함) ──────────────────────────────\n",
    "def crawl_images_by_category(crawl_targets, base_dir, max_per_variant=100):\n",
    "    for category, queries in crawl_targets.items():\n",
    "        category_folder = os.path.join(base_dir, category)\n",
    "        os.makedirs(category_folder, exist_ok=True)\n",
    "        print(f\"\\n=== 카테고리: {category} ===\")\n",
    "        # tqdm을 사용하여 각 검색어 진행 상황을 표시합니다.\n",
    "        for query in tqdm(queries, desc=f\"Processing {category}\", unit=\"query\"):\n",
    "            sub_folder = os.path.join(category_folder, query.replace(\" \", \"_\"))\n",
    "            os.makedirs(sub_folder, exist_ok=True)\n",
    "            print(f\"📦 크롤링: '{query}' → {sub_folder}\")\n",
    "            crawler = GoogleImageCrawler(storage={\"root_dir\": sub_folder})\n",
    "            crawler.crawl(\n",
    "                keyword=query,\n",
    "                max_num=max_per_variant,\n",
    "                filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "            )\n",
    "            resize_images_in_folder(sub_folder, resize_size)\n",
    "            remove_duplicate_images(sub_folder)\n",
    "            time.sleep(10)\n",
    "\n",
    "# ─── 함수: 클래스별 이미지 수 ──────────────────────────────\n",
    "def count_images_per_class(base_dir):\n",
    "    counts = {}\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        total = 0\n",
    "        for root, _, files in os.walk(category_path):\n",
    "            total += len([f for f in files if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "        counts[category] = total\n",
    "    return counts\n",
    "\n",
    "# ─── 메인 실행 부분 ──────────────────────────────\n",
    "crawl_images_by_category(modified_crawl_targets, base_dir, max_per_variant=300)\n",
    "\n",
    "# ─── 전체 실행 후, 각 카테고리별 이미지 수를 시각화 (막대그래프) ──────────────────────────────\n",
    "def visualize_image_counts(base_dir):\n",
    "    counts = count_images_per_class(base_dir)\n",
    "    categories = list(counts.keys())\n",
    "    image_counts = list(counts.values())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(categories, image_counts, color='skyblue')\n",
    "    plt.xlabel(\"카테고리\")\n",
    "    plt.ylabel(\"이미지 개수\")\n",
    "    plt.title(\"카테고리별 이미지 수\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_image_counts(base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd741c2",
   "metadata": {},
   "source": [
    "## 부족한 부분 재크롤링 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ccdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image, ImageStat\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import cv2\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "\n",
    "# 기존에 정의된 함수들 (resize_images_in_folder, is_blurry, is_mostly_black, remove_duplicate_images, crawl_trash_images)\n",
    "# 는 이미 포함되어 있다고 가정합니다.\n",
    "# ---------------------------------------------------\n",
    "# 아래는 재크롤링 및 데이터 정제 흐름에 필요한 추가 함수들입니다.\n",
    "\n",
    "# 1. 기존 이미지들의 해시값 수집\n",
    "def get_existing_hashes(base_dir):\n",
    "    existing_hashes = set()\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(root, fname)\n",
    "            try:\n",
    "                with Image.open(fpath) as img:\n",
    "                    h = imagehash.phash(img)\n",
    "                    existing_hashes.add(h)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return existing_hashes\n",
    "\n",
    "# 2. 기존 해시와 비교하여 중복 제거 (이미 수집된 해시 set을 업데이트하며 진행)\n",
    "def remove_duplicate_with_existing(folder_path, existing_hashes, hash_size=16):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                img_hash = imagehash.phash(img, hash_size=hash_size)\n",
    "            if img_hash in existing_hashes:\n",
    "                os.remove(fpath)\n",
    "                print(f\"🗑️ 기존 중복 제거: {fname}\")\n",
    "            else:\n",
    "                existing_hashes.add(img_hash)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 해시 실패: {fname} → {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "# 3. 각 클래스(카테고리)별 이미지 수 확인 함수\n",
    "def get_class_counts(base_dir):\n",
    "    counts = {}\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            num_images = len([\n",
    "                f for f in os.listdir(category_path)\n",
    "                if os.path.isfile(os.path.join(category_path, f))\n",
    "            ])\n",
    "            counts[category] = num_images\n",
    "    return counts\n",
    "\n",
    "# 4. 부족한 클래스 재크롤링을 위한 함수 (추가 modifier 활용)\n",
    "def recrawl_category_with_modifier(category, modifier, save_path, max_per_variant=50):\n",
    "    search_query = f\"{modifier} {category}\"\n",
    "    sub_folder = f\"{category.replace(' ', '_')}_{modifier.replace(' ', '_')}\"\n",
    "    output_path = os.path.join(save_path, sub_folder)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"📦 재크롤링: {search_query} → {output_path}\")\n",
    "    crawler = GoogleImageCrawler(storage={\"root_dir\": output_path})\n",
    "    crawler.crawl(\n",
    "        keyword=search_query,\n",
    "        max_num=max_per_variant,\n",
    "        filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "    )\n",
    "    \n",
    "    resize_images_in_folder(output_path, resize_size)\n",
    "    # 재크롤링 시 기존 해시와 비교하여 중복 이미지 제거\n",
    "    remove_duplicate_with_existing(output_path, existing_hashes)\n",
    "    time.sleep(10)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 이미 진행한 초기 크롤링 코드 (예시)\n",
    "for category in crawl_categories:\n",
    "    category_path = os.path.join(base_dir, category.replace(\" \", \"_\"))\n",
    "    crawl_trash_images(category, category_path, max_per_variant=100)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ① 기존 데이터셋의 모든 이미지 해시 수집\n",
    "existing_hashes = get_existing_hashes(base_dir)\n",
    "\n",
    "# ② 각 카테고리 폴더에서 기존 해시와 비교하여 중복 이미지 제거\n",
    "for category in crawl_categories:\n",
    "    category_folder = os.path.join(base_dir, category.replace(\" \", \"_\"))\n",
    "    if os.path.exists(category_folder):\n",
    "        remove_duplicate_with_existing(category_folder, existing_hashes)\n",
    "\n",
    "# ③ 클래스별 이미지 수 확인\n",
    "class_counts = get_class_counts(base_dir)\n",
    "print(\"현재 클래스별 이미지 수:\")\n",
    "for cat, count in class_counts.items():\n",
    "    print(f\"{cat}: {count}\")\n",
    "\n",
    "# ④ 부족한 클래스에 대해 재크롤링 수행 (예: 최소 200장 이상 확보)\n",
    "minimum_images = 200\n",
    "# 추가 크롤링을 위한 extra modifier 리스트\n",
    "extra_modifiers = [\"recycled\", \"old\", \"broken\", \"dirty\", \"used\", \"disposed\"]\n",
    "\n",
    "# 각 카테고리별 부족한 이미지 수에 대해 재크롤링 진행\n",
    "class_counts = get_class_counts(base_dir)\n",
    "for category, count in class_counts.items():\n",
    "    if count < minimum_images:\n",
    "        print(f\"카테고리 '{category}'의 이미지가 부족합니다 ({count}개). 재크롤링을 시작합니다.\")\n",
    "        category_folder = os.path.join(base_dir, category)\n",
    "        # 각 추가 modifier로 재크롤링 시도\n",
    "        for modifier in extra_modifiers:\n",
    "            recrawl_category_with_modifier(category, modifier, category_folder, max_per_variant=50)\n",
    "\n",
    "# ⑤ 최종 클래스별 이미지 수 확인\n",
    "final_counts = get_class_counts(base_dir)\n",
    "print(\"최종 클래스별 이미지 수:\")\n",
    "for cat, count in final_counts.items():\n",
    "    print(f\"{cat}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba1924",
   "metadata": {},
   "source": [
    "## 이미지 데이터셋 예측 후 분류를 위한 코드 (정확도 떨어짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4e5de",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# 데이터셋 경로 지정 (r\"\"를 사용하여 백슬래시를 이스케이프 처리)\n",
    "dataset_path = r\"C:\\Users\\Administrator\\Downloads\\capstone data\\DATASET\"\n",
    "\n",
    "# 폴더 내의 파일 목록 가져오기\n",
    "files = os.listdir(dataset_path)\n",
    "\n",
    "# 이미지 파일 확장자 리스트 (필요에 따라 추가)\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "\n",
    "# 이미지 파일만 필터링\n",
    "image_files = [f for f in files if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "\n",
    "print(\"총 이미지 개수:\", len(image_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213d5ee",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76ab90",
   "metadata": {},
   "source": [
    "# 사전 학습된 모델(ResNet50) 로드 및 평가 모드 전환\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e38e5",
   "metadata": {},
   "source": [
    "# 이미지 전처리 파이프라인 구성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5892c",
   "metadata": {},
   "source": [
    "# ImageNet 클래스 정보를 가져오기 위해 카테고리 맵을 다운로드 (예시)\n",
    "url, filename = (\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\", \"imagenet_class_index.json\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "with open('imagenet_class_index.json') as f:\n",
    "    class_idx = json.load(f)\n",
    "idx2label = {int(key): value[1] for key, value in class_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd3d3f",
   "metadata": {},
   "source": [
    "# 이미지가 저장된 폴더 경로 설정\n",
    "image_dir = r\"C:\\Users\\Administrator\\Downloads\\capstone data\\DATASET\\TRAIN\\R\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680eb7c",
   "metadata": {},
   "source": [
    "# 폴더 내 각 이미지에 대해 예측 실행\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        img_path = os.path.join(image_dir, image_file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        confidence, predicted_idx = torch.max(probabilities, dim=0)\n",
    "        predicted_label = idx2label[predicted_idx.item()]\n",
    "        results.append({\n",
    "            \"filename\": image_file,\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"confidence\": confidence.item()\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c93b9",
   "metadata": {},
   "source": [
    "# 예측 결과를 CSV로 저장\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e605c",
   "metadata": {},
   "source": [
    "# 예측 결과를 JSON으로 저장\n",
    "with open(\"predictions.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704b75f",
   "metadata": {},
   "source": [
    "## 예측 후 label studio 연결 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7689c0",
   "metadata": {},
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 미리 저장된 예측 결과 파일 불러오기\n",
    "# predictions.json 파일은 아래와 같이 구성했다고 가정합니다.\n",
    "# [\n",
    "#   {\"filename\": \"image1.jpg\", \"predicted_label\": \"wood\", \"confidence\": 0.95},\n",
    "#   {\"filename\": \"image2.jpg\", \"predicted_label\": \"plastic\", \"confidence\": 0.90},\n",
    "#   ...\n",
    "# ]\n",
    "with open('predictions.json', 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Label Studio에서 보내는 요청의 JSON 내용\n",
    "    input_data = request.get_json()\n",
    "    print(\"Received request:\", input_data)\n",
    "    \n",
    "    # Label Studio 요청 JSON에서 이미지 URL(또는 경로)를 추출합니다.\n",
    "    # 일반적으로는 task의 data 항목에 image URL이 포함되어 있습니다.\n",
    "    image_url = input_data.get('data', {}).get('image', '')\n",
    "    \n",
    "    # 파일명 추출: 이미지 URL이 http://.../image1.jpg 같은 형태라면 마지막 부분을 사용합니다.\n",
    "    filename = os.path.basename(image_url)\n",
    "    \n",
    "    # 예측 결과 탐색: predictions.json에 저장된 결과에서 파일명으로 매칭\n",
    "    result_payload = {}\n",
    "    for pred in predictions:\n",
    "        if pred.get(\"filename\") == filename:\n",
    "            # Label Studio가 이해할 수 있는 예측 포맷 생성\n",
    "            result_payload = {\n",
    "                \"result\": [\n",
    "                    {\n",
    "                        \"from_name\": \"label\",       # Label Studio에서 지정한 라벨 위젯의 이름\n",
    "                        \"to_name\": \"image\",         # 라벨링 대상(예: 이미지)의 이름\n",
    "                        \"type\": \"choices\",          # task 종류 (여기서는 단일 선택 분류)\n",
    "                        \"value\": {\n",
    "                            \"choices\": [pred.get(\"predicted_label\")]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            break\n",
    "\n",
    "    # 예측 결과가 없는 경우 빈 결과 반환\n",
    "    if not result_payload:\n",
    "        result_payload = {\"result\": []}\n",
    "    \n",
    "    print(\"Returning prediction:\", result_payload)\n",
    "    return jsonify(result_payload)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 서버를 0.0.0.0:5000에서 실행하여 Label Studio가 접근할 수 있도록 설정\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c1409",
   "metadata": {},
   "source": [
    "## 파일 분류 및 이동 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541685ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, shutil\n",
    "\n",
    "# 기존에 사용한 함수 (파일명에서 숫자를 추출하는 함수)\n",
    "def extract_idx(fname):\n",
    "    \"\"\"\n",
    "    파일명에서 R_123 또는 R123 형태로 된 숫자를 뽑아 정수로 반환.\n",
    "    매치되지 않으면 None 반환.\n",
    "    \"\"\"\n",
    "    base = os.path.splitext(fname)[0]  # ex. \"R_1\" 또는 \"R1\"\n",
    "    m = re.match(r\"^R_?(\\d+)$\", base)   # R 또는 R_ 뒤 숫자 캡처\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "# 기본 경로 설정 (필요에 따라 변경)\n",
    "dst_root = r\"C:\\Users\\Administrator\\Downloads\\capstone data\\DATASET\\TRAIN\"\n",
    "\n",
    "# paper 폴더 경로 (기존 분류 작업으로 생성된 폴더)\n",
    "paper_folder = os.path.join(dst_root, \"paper\")\n",
    "\n",
    "# 새로 생성할 book 폴더 경로\n",
    "book_folder = os.path.join(dst_root, \"book\")\n",
    "os.makedirs(book_folder, exist_ok=True)\n",
    "\n",
    "# paper 폴더에서 파일들을 순회하며 인덱스가 R_3579 ~ R_3849인 파일들을 book 폴더로 이동\n",
    "print(\"=== paper 폴더에서 book 파일 이동 시작 ===\")\n",
    "for fname in os.listdir(paper_folder):\n",
    "    idx = extract_idx(fname)\n",
    "    if idx is not None and 3579 <= idx <= 3849:\n",
    "        src_path = os.path.join(paper_folder, fname)\n",
    "        dst_path = os.path.join(book_folder, fname)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        print(f\"{fname} 이동 완료\")\n",
    "print(\"=== 이동 완료! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9dd33",
   "metadata": {},
   "source": [
    "## croma database 문서 입력 및 임베딩 후 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c012467",
   "metadata": {},
   "source": [
    "!pip install -U langchain-community\n",
    "!pip install langchain chromadb unstructured sentence-transformers docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a49593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"chroma_db\", exist_ok=True)\n",
    "\n",
    "# ---- corrected imports ----\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader, PyPDFLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import BaseRetriever   # ← moved here\n",
    "\n",
    "def ingest_to_chromadb(\n",
    "    source_path: str,\n",
    "    persist_directory: str = \"./chroma_db\",\n",
    "    collection_name: str = \"waste_policy\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    ") -> Chroma:\n",
    "    if source_path.lower().endswith(\".docx\"):\n",
    "        loader = UnstructuredWordDocumentLoader(source_path)\n",
    "    elif source_path.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(source_path)\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 파일 형식입니다. (.docx, .pdf 만 가능)\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\"Heading 1\",\"Heading 2\",\"Heading 3\"],\n",
    "    max_chunk_size=chunk_size,    # chunk_size → max_chunk_size 로 변경\n",
    "    chunk_overlap=chunk_overlap)   # 그대로 사용\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    return vectordb\n",
    "\n",
    "def get_simple_retriever(vectordb: Chroma, k: int = 4) -> BaseRetriever:\n",
    "    return vectordb.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": k}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f59d6",
   "metadata": {},
   "source": [
    "## Kaggle E-Waste Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6013a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/akshat103/e-waste-image-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.8M/11.8M [00:01<00:00, 9.00MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: C:\\Users\\Administrator\\.cache\\kagglehub\\datasets\\akshat103\\e-waste-image-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"akshat103/e-waste-image-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f36d0",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
