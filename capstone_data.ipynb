{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731b1641",
   "metadata": {},
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ Ïõπ ÌÅ¨Î°§ÎßÅ ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from PIL import Image, ImageStat\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resize_size = (512, 512)\n",
    "\n",
    "# --- Ïù¥ÎØ∏ÏßÄ ÌíàÏßà Î∞è ÌïÑÌÑ∞ Ìï®Ïàò ---\n",
    "def is_blurry(image, threshold=100):\n",
    "    try:\n",
    "        img = np.array(image)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        lap = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        return lap < threshold\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_mostly_black(image, threshold=15):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    avg_brightness = sum(stat.mean) / len(stat.mean)\n",
    "    return avg_brightness < threshold\n",
    "\n",
    "def resize_images_in_folder(folder_path, size):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath).convert(\"RGB\") as img:\n",
    "                if is_blurry(img) or is_mostly_black(img):\n",
    "                    os.remove(fpath)\n",
    "                    continue\n",
    "                img = img.resize(size)\n",
    "                new_fpath = os.path.splitext(fpath)[0] + \".jpg\"\n",
    "                img.save(new_fpath, \"JPEG\")\n",
    "                if fpath != new_fpath:\n",
    "                    os.remove(fpath)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Î¶¨ÏÇ¨Ïù¥Ïßï Ïã§Ìå®: {fname} ‚Üí {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "def remove_duplicate_images(folder_path, hash_size=16):\n",
    "    seen_hashes = set()\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                img_hash = imagehash.phash(img, hash_size=hash_size)\n",
    "            if img_hash in seen_hashes:\n",
    "                os.remove(fpath)\n",
    "                print(f\"üóëÔ∏è Ï§ëÎ≥µ Ï†úÍ±∞: {fname}\")\n",
    "            else:\n",
    "                seen_hashes.add(img_hash)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Ìï¥Ïãú Ïã§Ìå®: {fname} ‚Üí {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "# --- Ìè¥Îçî ÎÇ¥ Ïù¥ÎØ∏ÏßÄÏùò Ìï¥Ïãú Í∞íÎì§ÏùÑ Í≥ÑÏÇ∞ ---\n",
    "def compute_hashes_in_folder(folder_path, hash_size=16):\n",
    "    hashes = set()\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                h = imagehash.phash(img, hash_size=hash_size)\n",
    "                hashes.add(str(h))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Ìï¥Ïãú Í≥ÑÏÇ∞ Ïã§Ìå®: {fname} ‚Üí {e}\")\n",
    "    return hashes\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Î∂ÄÏ°±Ìïú ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌï¥ Ïû¨ÌÅ¨Î°§ÎßÅ (ÏûÑÏãú Ìè¥ÎçîÏóê ÌÅ¨Î°§ÎßÅ ÌõÑ Ï§ëÎ≥µ Ï≤¥ÌÅ¨) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def crawl_new_images_for_class(class_name, search_queries, main_folder, temp_folder, max_num=50, hash_size=16):\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    # Í∏∞Ï°¥ Ïù¥ÎØ∏ÏßÄÎì§Ïùò Ìï¥Ïãú Í≥ÑÏÇ∞\n",
    "    existing_hashes = compute_hashes_in_folder(main_folder, hash_size=hash_size)\n",
    "    \n",
    "    for query in search_queries:\n",
    "        query_folder = os.path.join(temp_folder, query.replace(\" \", \"_\"))\n",
    "        os.makedirs(query_folder, exist_ok=True)\n",
    "        print(f\"[{class_name}] ÏÉàÎ°úÏö¥ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Î°§ÎßÅ Ï§ë - Í≤ÄÏÉâÏñ¥: '{query}'\")\n",
    "        crawler = GoogleImageCrawler(storage={\"root_dir\": query_folder})\n",
    "        crawler.crawl(\n",
    "            keyword=query,\n",
    "            max_num=max_num,\n",
    "            filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "        )\n",
    "        \n",
    "        resize_images_in_folder(query_folder, resize_size)\n",
    "        \n",
    "        for fname in os.listdir(query_folder):\n",
    "            fpath = os.path.join(query_folder, fname)\n",
    "            try:\n",
    "                with Image.open(fpath) as img:\n",
    "                    h = str(imagehash.phash(img, hash_size=hash_size))\n",
    "                if h in existing_hashes:\n",
    "                    os.remove(fpath)\n",
    "                    print(f\"Ï§ëÎ≥µ Ïù¥ÎØ∏ÏßÄ Ï†úÍ±∞Îê®: {fname}\")\n",
    "                else:\n",
    "                    dest_path = os.path.join(main_folder, fname)\n",
    "                    shutil.move(fpath, dest_path)\n",
    "                    existing_hashes.add(h)\n",
    "            except Exception as e:\n",
    "                print(f\"Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Ïò§Î•ò {fname}: {e}\")\n",
    "        \n",
    "        if not os.listdir(query_folder):\n",
    "            os.rmdir(query_folder)\n",
    "        time.sleep(5)\n",
    "\n",
    "    remove_duplicate_images(main_folder, hash_size=hash_size)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ ÌÅ¨Î°§ ÎåÄÏÉÅ ÌÅ¥ÎûòÏä§ (Í≤ÄÏÉâÏñ¥ Î¶¨Ïä§Ìä∏) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "crawl_targets = {\n",
    "    \"wood\": [\"wood\", \"plywood pieces\", \"furniture wood pieces\"],\n",
    "    \"food_waste\": [\n",
    "        \"egg shells\", \"duck egg shells\", \"quail egg shells\", \"ostrich egg shells\",\n",
    "        \"walnut shells\", \"peanut shells\", \"chestnut shells\", \"acorn shells\",\n",
    "        \"pineapple peels\", \"corn husks\", \"corn cobs\",\n",
    "        \"green tea leaves\", \"herbal medicine residue\",\n",
    "        \"pork bones\", \"beef bones\", \"chicken bones\", \"fish bones\",\n",
    "        \"clam shells\", \"crab shells\", \"lobster shells\",\n",
    "        \"rice husks\", \"food waste\", \"onion peels\"\n",
    "    ],\n",
    "    \"general_waste\": [\"ceramic dishes\", \"porcelain items\"],\n",
    "    \"paper\": [\"paper packs\", \"paper cups\", \"newspapers\", \"books\", \"notebooks\", \"cardboard boxes\"],\n",
    "    \"glass\": [\"glass bottles\", \"broken glass\"],\n",
    "    \"can\": [\"steel cans\", \"aluminum cans\", \"butane gas cans\", \"pesticide cans\"],\n",
    "    \"plastic\": [\"clear PET bottles\", \"colored PET bottles\", \"plastic bags\"],\n",
    "    \"styrofoam\": [\"styrofoam\", \"contaminated styrofoam\"],\n",
    "    \"battery\": [\"batteries\", \"AA batteries\", \"AAA batteries\"],\n",
    "    \"electronics\": [\"TVs\", \"refrigerators\", \"washing machines\", \"air conditioners\", \"computers\", \"mobile phones\"],\n",
    "    \"lighting\": [\"fluorescent lamps\"],\n",
    "    \"metal\": [\"scrap metal\", \"iron pipes\"],\n",
    "    \"clothing\": [\"clothes\", \"old clothes\"]\n",
    "}\n",
    "\n",
    "# Í∞Å Í≤ÄÏÉâÏñ¥ Î¨∏ÏûêÏó¥Ïùò ÎÅùÏóê \" waste img\"Î•º Ï∂îÍ∞ÄÌïú ÏÉàÎ°úÏö¥ ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ±\n",
    "modified_crawl_targets = {\n",
    "    category: [f\"{query} waste img\" for query in queries]\n",
    "    for category, queries in crawl_targets.items()\n",
    "}\n",
    "\n",
    "print(modified_crawl_targets)\n",
    "\n",
    "base_dir = r\"C:\\Users\\Administrator\\Downloads\\end-to-end-image-scraper\\downloaded_images\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Î°§ÎßÅ Ìï®Ïàò (ÏßÑÌñâ Î∞î Ìè¨Ìï®) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def crawl_images_by_category(crawl_targets, base_dir, max_per_variant=100):\n",
    "    for category, queries in crawl_targets.items():\n",
    "        category_folder = os.path.join(base_dir, category)\n",
    "        os.makedirs(category_folder, exist_ok=True)\n",
    "        print(f\"\\n=== Ïπ¥ÌÖåÍ≥†Î¶¨: {category} ===\")\n",
    "        # tqdmÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í∞Å Í≤ÄÏÉâÏñ¥ ÏßÑÌñâ ÏÉÅÌô©ÏùÑ ÌëúÏãúÌï©ÎãàÎã§.\n",
    "        for query in tqdm(queries, desc=f\"Processing {category}\", unit=\"query\"):\n",
    "            sub_folder = os.path.join(category_folder, query.replace(\" \", \"_\"))\n",
    "            os.makedirs(sub_folder, exist_ok=True)\n",
    "            print(f\"üì¶ ÌÅ¨Î°§ÎßÅ: '{query}' ‚Üí {sub_folder}\")\n",
    "            crawler = GoogleImageCrawler(storage={\"root_dir\": sub_folder})\n",
    "            crawler.crawl(\n",
    "                keyword=query,\n",
    "                max_num=max_per_variant,\n",
    "                filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "            )\n",
    "            resize_images_in_folder(sub_folder, resize_size)\n",
    "            remove_duplicate_images(sub_folder)\n",
    "            time.sleep(10)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Ìï®Ïàò: ÌÅ¥ÎûòÏä§Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def count_images_per_class(base_dir):\n",
    "    counts = {}\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        total = 0\n",
    "        for root, _, files in os.walk(category_path):\n",
    "            total += len([f for f in files if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "        counts[category] = total\n",
    "    return counts\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Î©îÏù∏ Ïã§Ìñâ Î∂ÄÎ∂Ñ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "crawl_images_by_category(modified_crawl_targets, base_dir, max_per_variant=300)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Ï†ÑÏ≤¥ Ïã§Ìñâ ÌõÑ, Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Ïù¥ÎØ∏ÏßÄ ÏàòÎ•º ÏãúÍ∞ÅÌôî (ÎßâÎåÄÍ∑∏ÎûòÌîÑ) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def visualize_image_counts(base_dir):\n",
    "    counts = count_images_per_class(base_dir)\n",
    "    categories = list(counts.keys())\n",
    "    image_counts = list(counts.values())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(categories, image_counts, color='skyblue')\n",
    "    plt.xlabel(\"Ïπ¥ÌÖåÍ≥†Î¶¨\")\n",
    "    plt.ylabel(\"Ïù¥ÎØ∏ÏßÄ Í∞úÏàò\")\n",
    "    plt.title(\"Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_image_counts(base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd741c2",
   "metadata": {},
   "source": [
    "## Î∂ÄÏ°±Ìïú Î∂ÄÎ∂Ñ Ïû¨ÌÅ¨Î°§ÎßÅ ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ccdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image, ImageStat\n",
    "import imagehash\n",
    "import numpy as np\n",
    "import cv2\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "\n",
    "# Í∏∞Ï°¥Ïóê Ï†ïÏùòÎêú Ìï®ÏàòÎì§ (resize_images_in_folder, is_blurry, is_mostly_black, remove_duplicate_images, crawl_trash_images)\n",
    "# Îäî Ïù¥ÎØ∏ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎã§Í≥† Í∞ÄÏ†ïÌï©ÎãàÎã§.\n",
    "# ---------------------------------------------------\n",
    "# ÏïÑÎûòÎäî Ïû¨ÌÅ¨Î°§ÎßÅ Î∞è Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú ÌùêÎ¶ÑÏóê ÌïÑÏöîÌïú Ï∂îÍ∞Ä Ìï®ÏàòÎì§ÏûÖÎãàÎã§.\n",
    "\n",
    "# 1. Í∏∞Ï°¥ Ïù¥ÎØ∏ÏßÄÎì§Ïùò Ìï¥ÏãúÍ∞í ÏàòÏßë\n",
    "def get_existing_hashes(base_dir):\n",
    "    existing_hashes = set()\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(root, fname)\n",
    "            try:\n",
    "                with Image.open(fpath) as img:\n",
    "                    h = imagehash.phash(img)\n",
    "                    existing_hashes.add(h)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return existing_hashes\n",
    "\n",
    "# 2. Í∏∞Ï°¥ Ìï¥ÏãúÏôÄ ÎπÑÍµêÌïòÏó¨ Ï§ëÎ≥µ Ï†úÍ±∞ (Ïù¥ÎØ∏ ÏàòÏßëÎêú Ìï¥Ïãú setÏùÑ ÏóÖÎç∞Ïù¥Ìä∏ÌïòÎ©∞ ÏßÑÌñâ)\n",
    "def remove_duplicate_with_existing(folder_path, existing_hashes, hash_size=16):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            with Image.open(fpath) as img:\n",
    "                img_hash = imagehash.phash(img, hash_size=hash_size)\n",
    "            if img_hash in existing_hashes:\n",
    "                os.remove(fpath)\n",
    "                print(f\"üóëÔ∏è Í∏∞Ï°¥ Ï§ëÎ≥µ Ï†úÍ±∞: {fname}\")\n",
    "            else:\n",
    "                existing_hashes.add(img_hash)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Ìï¥Ïãú Ïã§Ìå®: {fname} ‚Üí {e}\")\n",
    "            os.remove(fpath)\n",
    "\n",
    "# 3. Í∞Å ÌÅ¥ÎûòÏä§(Ïπ¥ÌÖåÍ≥†Î¶¨)Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò ÌôïÏù∏ Ìï®Ïàò\n",
    "def get_class_counts(base_dir):\n",
    "    counts = {}\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            num_images = len([\n",
    "                f for f in os.listdir(category_path)\n",
    "                if os.path.isfile(os.path.join(category_path, f))\n",
    "            ])\n",
    "            counts[category] = num_images\n",
    "    return counts\n",
    "\n",
    "# 4. Î∂ÄÏ°±Ìïú ÌÅ¥ÎûòÏä§ Ïû¨ÌÅ¨Î°§ÎßÅÏùÑ ÏúÑÌïú Ìï®Ïàò (Ï∂îÍ∞Ä modifier ÌôúÏö©)\n",
    "def recrawl_category_with_modifier(category, modifier, save_path, max_per_variant=50):\n",
    "    search_query = f\"{modifier} {category}\"\n",
    "    sub_folder = f\"{category.replace(' ', '_')}_{modifier.replace(' ', '_')}\"\n",
    "    output_path = os.path.join(save_path, sub_folder)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"üì¶ Ïû¨ÌÅ¨Î°§ÎßÅ: {search_query} ‚Üí {output_path}\")\n",
    "    crawler = GoogleImageCrawler(storage={\"root_dir\": output_path})\n",
    "    crawler.crawl(\n",
    "        keyword=search_query,\n",
    "        max_num=max_per_variant,\n",
    "        filters={\"type\": \"photo\", \"size\": \"large\"}\n",
    "    )\n",
    "    \n",
    "    resize_images_in_folder(output_path, resize_size)\n",
    "    # Ïû¨ÌÅ¨Î°§ÎßÅ Ïãú Í∏∞Ï°¥ Ìï¥ÏãúÏôÄ ÎπÑÍµêÌïòÏó¨ Ï§ëÎ≥µ Ïù¥ÎØ∏ÏßÄ Ï†úÍ±∞\n",
    "    remove_duplicate_with_existing(output_path, existing_hashes)\n",
    "    time.sleep(10)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Ïù¥ÎØ∏ ÏßÑÌñâÌïú Ï¥àÍ∏∞ ÌÅ¨Î°§ÎßÅ ÏΩîÎìú (ÏòàÏãú)\n",
    "for category in crawl_categories:\n",
    "    category_path = os.path.join(base_dir, category.replace(\" \", \"_\"))\n",
    "    crawl_trash_images(category, category_path, max_per_variant=100)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ‚ë† Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Î™®Îì† Ïù¥ÎØ∏ÏßÄ Ìï¥Ïãú ÏàòÏßë\n",
    "existing_hashes = get_existing_hashes(base_dir)\n",
    "\n",
    "# ‚ë° Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨ Ìè¥ÎçîÏóêÏÑú Í∏∞Ï°¥ Ìï¥ÏãúÏôÄ ÎπÑÍµêÌïòÏó¨ Ï§ëÎ≥µ Ïù¥ÎØ∏ÏßÄ Ï†úÍ±∞\n",
    "for category in crawl_categories:\n",
    "    category_folder = os.path.join(base_dir, category.replace(\" \", \"_\"))\n",
    "    if os.path.exists(category_folder):\n",
    "        remove_duplicate_with_existing(category_folder, existing_hashes)\n",
    "\n",
    "# ‚ë¢ ÌÅ¥ÎûòÏä§Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò ÌôïÏù∏\n",
    "class_counts = get_class_counts(base_dir)\n",
    "print(\"ÌòÑÏû¨ ÌÅ¥ÎûòÏä§Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò:\")\n",
    "for cat, count in class_counts.items():\n",
    "    print(f\"{cat}: {count}\")\n",
    "\n",
    "# ‚ë£ Î∂ÄÏ°±Ìïú ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌï¥ Ïû¨ÌÅ¨Î°§ÎßÅ ÏàòÌñâ (Ïòà: ÏµúÏÜå 200Ïû• Ïù¥ÏÉÅ ÌôïÎ≥¥)\n",
    "minimum_images = 200\n",
    "# Ï∂îÍ∞Ä ÌÅ¨Î°§ÎßÅÏùÑ ÏúÑÌïú extra modifier Î¶¨Ïä§Ìä∏\n",
    "extra_modifiers = [\"recycled\", \"old\", \"broken\", \"dirty\", \"used\", \"disposed\"]\n",
    "\n",
    "# Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Î∂ÄÏ°±Ìïú Ïù¥ÎØ∏ÏßÄ ÏàòÏóê ÎåÄÌï¥ Ïû¨ÌÅ¨Î°§ÎßÅ ÏßÑÌñâ\n",
    "class_counts = get_class_counts(base_dir)\n",
    "for category, count in class_counts.items():\n",
    "    if count < minimum_images:\n",
    "        print(f\"Ïπ¥ÌÖåÍ≥†Î¶¨ '{category}'Ïùò Ïù¥ÎØ∏ÏßÄÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§ ({count}Í∞ú). Ïû¨ÌÅ¨Î°§ÎßÅÏùÑ ÏãúÏûëÌï©ÎãàÎã§.\")\n",
    "        category_folder = os.path.join(base_dir, category)\n",
    "        # Í∞Å Ï∂îÍ∞Ä modifierÎ°ú Ïû¨ÌÅ¨Î°§ÎßÅ ÏãúÎèÑ\n",
    "        for modifier in extra_modifiers:\n",
    "            recrawl_category_with_modifier(category, modifier, category_folder, max_per_variant=50)\n",
    "\n",
    "# ‚ë§ ÏµúÏ¢Ö ÌÅ¥ÎûòÏä§Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò ÌôïÏù∏\n",
    "final_counts = get_class_counts(base_dir)\n",
    "print(\"ÏµúÏ¢Ö ÌÅ¥ÎûòÏä§Î≥Ñ Ïù¥ÎØ∏ÏßÄ Ïàò:\")\n",
    "for cat, count in final_counts.items():\n",
    "    print(f\"{cat}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba1924",
   "metadata": {},
   "source": [
    "## Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ÏÖã ÏòàÏ∏° ÌõÑ Î∂ÑÎ•òÎ•º ÏúÑÌïú ÏΩîÎìú (Ï†ïÌôïÎèÑ Îñ®Ïñ¥Ïßê)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4e5de",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú ÏßÄÏ†ï (r\"\"Î•º ÏÇ¨Ïö©ÌïòÏó¨ Î∞±Ïä¨ÎûòÏãúÎ•º Ïù¥Ïä§ÏºÄÏù¥ÌîÑ Ï≤òÎ¶¨)\n",
    "dataset_path = r\"C:\\Users\\Administrator\\Downloads\\capstone data\\DATASET\"\n",
    "\n",
    "# Ìè¥Îçî ÎÇ¥Ïùò ÌååÏùº Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "files = os.listdir(dataset_path)\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌååÏùº ÌôïÏû•Ïûê Î¶¨Ïä§Ìä∏ (ÌïÑÏöîÏóê Îî∞Îùº Ï∂îÍ∞Ä)\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "\n",
    "# Ïù¥ÎØ∏ÏßÄ ÌååÏùºÎßå ÌïÑÌÑ∞ÎßÅ\n",
    "image_files = [f for f in files if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "\n",
    "print(\"Ï¥ù Ïù¥ÎØ∏ÏßÄ Í∞úÏàò:\", len(image_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213d5ee",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76ab90",
   "metadata": {},
   "source": [
    "# ÏÇ¨Ï†Ñ ÌïôÏäµÎêú Î™®Îç∏(ResNet50) Î°úÎìú Î∞è ÌèâÍ∞Ä Î™®Îìú Ï†ÑÌôò\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e38e5",
   "metadata": {},
   "source": [
    "# Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨ÏÑ±\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5892c",
   "metadata": {},
   "source": [
    "# ImageNet ÌÅ¥ÎûòÏä§ Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò§Í∏∞ ÏúÑÌï¥ Ïπ¥ÌÖåÍ≥†Î¶¨ ÎßµÏùÑ Îã§Ïö¥Î°úÎìú (ÏòàÏãú)\n",
    "url, filename = (\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\", \"imagenet_class_index.json\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "with open('imagenet_class_index.json') as f:\n",
    "    class_idx = json.load(f)\n",
    "idx2label = {int(key): value[1] for key, value in class_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd3d3f",
   "metadata": {},
   "source": [
    "# Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï†ÄÏû•Îêú Ìè¥Îçî Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "image_dir = r\"C:\\Users\\Administrator\\Downloads\\capstone data\\DATASET\\TRAIN\\R\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680eb7c",
   "metadata": {},
   "source": [
    "# Ìè¥Îçî ÎÇ¥ Í∞Å Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌï¥ ÏòàÏ∏° Ïã§Ìñâ\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        img_path = os.path.join(image_dir, image_file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = transform(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        confidence, predicted_idx = torch.max(probabilities, dim=0)\n",
    "        predicted_label = idx2label[predicted_idx.item()]\n",
    "        results.append({\n",
    "            \"filename\": image_file,\n",
    "            \"predicted_label\": predicted_label,\n",
    "            \"confidence\": confidence.item()\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c93b9",
   "metadata": {},
   "source": [
    "# ÏòàÏ∏° Í≤∞Í≥ºÎ•º CSVÎ°ú Ï†ÄÏû•\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e605c",
   "metadata": {},
   "source": [
    "# ÏòàÏ∏° Í≤∞Í≥ºÎ•º JSONÏúºÎ°ú Ï†ÄÏû•\n",
    "with open(\"predictions.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704b75f",
   "metadata": {},
   "source": [
    "## ÏòàÏ∏° ÌõÑ label studio Ïó∞Í≤∞ ÏãúÎèÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7689c0",
   "metadata": {},
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# ÎØ∏Î¶¨ Ï†ÄÏû•Îêú ÏòàÏ∏° Í≤∞Í≥º ÌååÏùº Î∂àÎü¨Ïò§Í∏∞\n",
    "# predictions.json ÌååÏùºÏùÄ ÏïÑÎûòÏôÄ Í∞ôÏù¥ Íµ¨ÏÑ±ÌñàÎã§Í≥† Í∞ÄÏ†ïÌï©ÎãàÎã§.\n",
    "# [\n",
    "#   {\"filename\": \"image1.jpg\", \"predicted_label\": \"wood\", \"confidence\": 0.95},\n",
    "#   {\"filename\": \"image2.jpg\", \"predicted_label\": \"plastic\", \"confidence\": 0.90},\n",
    "#   ...\n",
    "# ]\n",
    "with open('predictions.json', 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Label StudioÏóêÏÑú Î≥¥ÎÇ¥Îäî ÏöîÏ≤≠Ïùò JSON ÎÇ¥Ïö©\n",
    "    input_data = request.get_json()\n",
    "    print(\"Received request:\", input_data)\n",
    "    \n",
    "    # Label Studio ÏöîÏ≤≠ JSONÏóêÏÑú Ïù¥ÎØ∏ÏßÄ URL(ÎòêÎäî Í≤ΩÎ°ú)Î•º Ï∂îÏ∂úÌï©ÎãàÎã§.\n",
    "    # ÏùºÎ∞òÏ†ÅÏúºÎ°úÎäî taskÏùò data Ìï≠Î™©Ïóê image URLÏù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n",
    "    image_url = input_data.get('data', {}).get('image', '')\n",
    "    \n",
    "    # ÌååÏùºÎ™Ö Ï∂îÏ∂ú: Ïù¥ÎØ∏ÏßÄ URLÏù¥ http://.../image1.jpg Í∞ôÏùÄ ÌòïÌÉúÎùºÎ©¥ ÎßàÏßÄÎßâ Î∂ÄÎ∂ÑÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "    filename = os.path.basename(image_url)\n",
    "    \n",
    "    # ÏòàÏ∏° Í≤∞Í≥º ÌÉêÏÉâ: predictions.jsonÏóê Ï†ÄÏû•Îêú Í≤∞Í≥ºÏóêÏÑú ÌååÏùºÎ™ÖÏúºÎ°ú Îß§Ïπ≠\n",
    "    result_payload = {}\n",
    "    for pred in predictions:\n",
    "        if pred.get(\"filename\") == filename:\n",
    "            # Label StudioÍ∞Ä Ïù¥Ìï¥Ìï† Ïàò ÏûàÎäî ÏòàÏ∏° Ìè¨Îß∑ ÏÉùÏÑ±\n",
    "            result_payload = {\n",
    "                \"result\": [\n",
    "                    {\n",
    "                        \"from_name\": \"label\",       # Label StudioÏóêÏÑú ÏßÄÏ†ïÌïú ÎùºÎ≤® ÏúÑÏ†ØÏùò Ïù¥Î¶Ñ\n",
    "                        \"to_name\": \"image\",         # ÎùºÎ≤®ÎßÅ ÎåÄÏÉÅ(Ïòà: Ïù¥ÎØ∏ÏßÄ)Ïùò Ïù¥Î¶Ñ\n",
    "                        \"type\": \"choices\",          # task Ï¢ÖÎ•ò (Ïó¨Í∏∞ÏÑúÎäî Îã®Ïùº ÏÑ†ÌÉù Î∂ÑÎ•ò)\n",
    "                        \"value\": {\n",
    "                            \"choices\": [pred.get(\"predicted_label\")]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            break\n",
    "\n",
    "    # ÏòàÏ∏° Í≤∞Í≥ºÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ Îπà Í≤∞Í≥º Î∞òÌôò\n",
    "    if not result_payload:\n",
    "        result_payload = {\"result\": []}\n",
    "    \n",
    "    print(\"Returning prediction:\", result_payload)\n",
    "    return jsonify(result_payload)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ÏÑúÎ≤ÑÎ•º 0.0.0.0:5000ÏóêÏÑú Ïã§ÌñâÌïòÏó¨ Label StudioÍ∞Ä Ï†ëÍ∑ºÌï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Ï†ï\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c1409",
   "metadata": {},
   "source": [
    "## ÌååÏùº Î∂ÑÎ•ò Î∞è Ïù¥Îèô ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541685ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, shutil\n",
    "\n",
    "# Í∏∞Ï°¥Ïóê ÏÇ¨Ïö©Ìïú Ìï®Ïàò (ÌååÏùºÎ™ÖÏóêÏÑú Ïà´ÏûêÎ•º Ï∂îÏ∂úÌïòÎäî Ìï®Ïàò)\n",
    "def extract_idx(fname):\n",
    "    \"\"\"\n",
    "    ÌååÏùºÎ™ÖÏóêÏÑú R_123 ÎòêÎäî R123 ÌòïÌÉúÎ°ú Îêú Ïà´ÏûêÎ•º ÎΩëÏïÑ Ï†ïÏàòÎ°ú Î∞òÌôò.\n",
    "    Îß§ÏπòÎêòÏßÄ ÏïäÏúºÎ©¥ None Î∞òÌôò.\n",
    "    \"\"\"\n",
    "    base = os.path.splitext(fname)[0]  # ex. \"R_1\" ÎòêÎäî \"R1\"\n",
    "    m = re.match(r\"^R_?(\\d+)$\", base)   # R ÎòêÎäî R_ Îí§ Ïà´Ïûê Ï∫°Ï≤ò\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "# Í∏∞Î≥∏ Í≤ΩÎ°ú ÏÑ§Ï†ï (ÌïÑÏöîÏóê Îî∞Îùº Î≥ÄÍ≤Ω)\n",
    "dst_root = r\"C:\\Users\\Administrator\\Downloads\\capstone data\\DATASET\\TRAIN\"\n",
    "\n",
    "# paper Ìè¥Îçî Í≤ΩÎ°ú (Í∏∞Ï°¥ Î∂ÑÎ•ò ÏûëÏóÖÏúºÎ°ú ÏÉùÏÑ±Îêú Ìè¥Îçî)\n",
    "paper_folder = os.path.join(dst_root, \"paper\")\n",
    "\n",
    "# ÏÉàÎ°ú ÏÉùÏÑ±Ìï† book Ìè¥Îçî Í≤ΩÎ°ú\n",
    "book_folder = os.path.join(dst_root, \"book\")\n",
    "os.makedirs(book_folder, exist_ok=True)\n",
    "\n",
    "# paper Ìè¥ÎçîÏóêÏÑú ÌååÏùºÎì§ÏùÑ ÏàúÌöåÌïòÎ©∞ Ïù∏Îç±Ïä§Í∞Ä R_3579 ~ R_3849Ïù∏ ÌååÏùºÎì§ÏùÑ book Ìè¥ÎçîÎ°ú Ïù¥Îèô\n",
    "print(\"=== paper Ìè¥ÎçîÏóêÏÑú book ÌååÏùº Ïù¥Îèô ÏãúÏûë ===\")\n",
    "for fname in os.listdir(paper_folder):\n",
    "    idx = extract_idx(fname)\n",
    "    if idx is not None and 3579 <= idx <= 3849:\n",
    "        src_path = os.path.join(paper_folder, fname)\n",
    "        dst_path = os.path.join(book_folder, fname)\n",
    "        shutil.move(src_path, dst_path)\n",
    "        print(f\"{fname} Ïù¥Îèô ÏôÑÎ£å\")\n",
    "print(\"=== Ïù¥Îèô ÏôÑÎ£å! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9dd33",
   "metadata": {},
   "source": [
    "## croma database Î¨∏ÏÑú ÏûÖÎ†• Î∞è ÏûÑÎ≤†Îî© ÌõÑ Ìò∏Ï∂ú"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c012467",
   "metadata": {},
   "source": [
    "!pip install -U langchain-community\n",
    "!pip install langchain chromadb unstructured sentence-transformers docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a49593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"chroma_db\", exist_ok=True)\n",
    "\n",
    "# ---- corrected imports ----\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader, PyPDFLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import BaseRetriever   # ‚Üê moved here\n",
    "\n",
    "def ingest_to_chromadb(\n",
    "    source_path: str,\n",
    "    persist_directory: str = \"./chroma_db\",\n",
    "    collection_name: str = \"waste_policy\",\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    ") -> Chroma:\n",
    "    if source_path.lower().endswith(\".docx\"):\n",
    "        loader = UnstructuredWordDocumentLoader(source_path)\n",
    "    elif source_path.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(source_path)\n",
    "    else:\n",
    "        raise ValueError(\"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌååÏùº ÌòïÏãùÏûÖÎãàÎã§. (.docx, .pdf Îßå Í∞ÄÎä•)\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\"Heading 1\",\"Heading 2\",\"Heading 3\"],\n",
    "    max_chunk_size=chunk_size,    # chunk_size ‚Üí max_chunk_size Î°ú Î≥ÄÍ≤Ω\n",
    "    chunk_overlap=chunk_overlap)   # Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    return vectordb\n",
    "\n",
    "def get_simple_retriever(vectordb: Chroma, k: int = 4) -> BaseRetriever:\n",
    "    return vectordb.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": k}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f59d6",
   "metadata": {},
   "source": [
    "## Kaggle E-Waste Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6013a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/akshat103/e-waste-image-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.8M/11.8M [00:01<00:00, 9.00MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: C:\\Users\\Administrator\\.cache\\kagglehub\\datasets\\akshat103\\e-waste-image-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"akshat103/e-waste-image-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f36d0",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
