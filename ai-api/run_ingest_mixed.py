import os
import sys
import uuid
import django
from pathlib import Path
from dotenv import load_dotenv
import re

from langchain_community.document_loaders import Docx2txtLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# Django ì´ˆê¸°í™”
sys.path.append(os.path.abspath("."))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "config.settings")
django.setup()

from SQLDB.models import VectorMetadata

# í™˜ê²½ë³€ìˆ˜ ë° ê²½ë¡œ ì„¤ì •
load_dotenv()
DOCUMENT_DIR = Path("/home/ubuntu/policy_file")
PERSIST_DIR = "./chroma_db"
COLLECTION = "waste_vectors"
os.makedirs(PERSIST_DIR, exist_ok=True)

# ----------- ì§€ì—­ ì¶”ì¶œ í•¨ìˆ˜ -----------
def extract_region_from_filename(filename: str) -> str:
    return os.path.splitext(filename)[0].split("_")[0].strip()

# ----------- ì¹´í…Œê³ ë¦¬ ë¸”ë¡ ë¶„ë¦¬ í•¨ìˆ˜ -----------
def split_by_category(doc_text):
    # [Category] ... end ë¸”ë¡ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°
    pattern = r"(\[Category\][\s\S]*?end)"
    blocks = re.findall(pattern, doc_text)
    return blocks

# ----------- ì¹´í…Œê³ ë¦¬ëª… ì¶”ì¶œ í•¨ìˆ˜ -----------
def extract_category_label(block):
    match = re.search(r"\[Category\]\s*([^\n\r]+)", block)
    if match:
        return match.group(1).strip()
    else:
        return "ê¸°íƒ€"

# ----------- ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” -----------
hf_embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"batch_size": 5}
)

vectordb = Chroma(
    embedding_function=hf_embeddings,
    persist_directory=PERSIST_DIR,
    collection_name=COLLECTION
)

# ----------- íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ -----------
doc_paths = list(DOCUMENT_DIR.glob("*.docx")) + list(DOCUMENT_DIR.glob("*.DOCX"))

print(f"\nğŸ“‚ ë¬¸ì„œ íƒìƒ‰ ê²½ë¡œ: {DOCUMENT_DIR}")
print(f"ğŸ” ë¬¸ì„œ íŒŒì¼ ìˆ˜: {len(doc_paths)}")

for file_path in doc_paths:
    try:
        print(f"\nğŸ“„ Processing: {file_path.name}")
        region = extract_region_from_filename(file_path.name)

        loader = Docx2txtLoader(str(file_path))
        docs = loader.load()
        
        # --- í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ ---
        if isinstance(docs, list):
            docs = "\n".join([d.page_content if isinstance(d, Document) else str(d) for d in docs])
        elif isinstance(docs, Document):
            docs = docs.page_content
        elif isinstance(docs, str):
            docs = docs
        else:
            raise ValueError("ë¬¸ì„œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # --- ì¹´í…Œê³ ë¦¬ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ ---
        category_blocks = split_by_category(docs)
        print(f"ì¹´í…Œê³ ë¦¬ ë¸”ë¡ ê°œìˆ˜: {len(category_blocks)}")
        
        # --- ì¹´í…Œê³ ë¦¬ëª… ì¶”ì¶œ ë° ì„ë² ë”©ìš© chunk ë§Œë“¤ê¸° ---
        ids = [str(uuid.uuid4()) for _ in category_blocks]
        documents = []
        for block, vector_id in zip(category_blocks, ids):
            label = extract_category_label(block)   # [Category] ë’·ë¶€ë¶„ì„ labelë¡œ!
            metadata = {
                "type": "guideline",
                "label": label,                     # label: [Category] ëª…ì¹­ ê·¸ëŒ€ë¡œ
                "region": region,
                "source_file": file_path.name
            }
            documents.append(Document(page_content=block, metadata=metadata))

        print(f"ìµœì¢… chunk ê°œìˆ˜: {len(documents)}")
        for idx, doc in enumerate(documents[:5], 1):
            print(f"\nchunk {idx} (label: {doc.metadata['label']}):\n{doc.page_content[:200]}\n---")

        # --- ì„ë² ë”© ë° DB ì €ì¥ ---
        vectordb.add_documents(documents=documents, ids=ids)
        for vector_id in ids:
            VectorMetadata.objects.create(vector_id=vector_id)

        print(f"âœ… {file_path.name}: {len(documents)} chunks ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {file_path.name} - {str(e)}")

print("\nğŸ“Œ ChromaDBì— ì €ì¥ëœ ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ í™•ì¸:")
try:
    raw = vectordb._collection.get(include=["metadatas", "documents"], limit=5)
    for i, (meta, doc) in enumerate(zip(raw["metadatas"], raw["documents"]), 1):
        print(f"\nğŸ§© meta {i}: {meta}")
        print(f"ğŸ“ doc {i}: {doc[:120]}...")
except Exception as e:
    print(f"âš ï¸ ë©”íƒ€ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {str(e)}")

